# -*- coding: utf-8 -*-
"""GNN Project reults

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Vpb12z2i_o4i-QblfCFRVjZFS1jb87mN
"""

!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu
!pip install torch-scatter -f https://data.pyg.org/whl/torch-2.0.0+cpu.html
!pip install torch-sparse -f https://data.pyg.org/whl/torch-2.0.0+cpu.html
!pip install torch-cluster -f https://data.pyg.org/whl/torch-2.0.0+cpu.html
!pip install torch-spline-conv -f https://data.pyg.org/whl/torch-2.0.0+cpu.html
!pip install torch-geometric
!pip install networkx tqdm matplotlib numpy pandas scikit-learn

import torch
import torch.nn.functional as F
from torch_geometric.datasets import Planetoid
from torch_geometric.nn import GCNConv
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import networkx as nx
import os

dataset = Planetoid(root='/tmp/Cora', name='Cora')
data = dataset[0]

# === STEP 4: Define a Basic GCN Model (baseline) ===
class GCN(torch.nn.Module):
    def __init__(self):
        super(GCN, self).__init__()
        self.conv1 = GCNConv(dataset.num_node_features, 16)
        self.conv2 = GCNConv(16, dataset.num_classes)

    def forward(self, data):
        x, edge_index = data.x, data.edge_index
        x = F.relu(self.conv1(x, edge_index))
        x = F.dropout(x, training=self.training)
        x = self.conv2(x, edge_index)
        return F.log_softmax(x, dim=1)

# === STEP 5: Train the Model ===
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model = GCN().to(device)
data = data.to(device)
optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)

def train():
    model.train()
    optimizer.zero_grad()
    out = model(data)
    loss = F.nll_loss(out[data.train_mask], data.y[data.train_mask])
    loss.backward()
    optimizer.step()
    return loss.item()

def test():
    model.eval()
    logits = model(data)
    masks = ['train_mask', 'val_mask', 'test_mask']
    accs = []
    for mask in masks:
        mask_idx = data[mask]
        pred = logits[mask_idx].max(1)[1]
        acc = pred.eq(data.y[mask_idx]).sum().item() / mask_idx.sum().item()
        accs.append(acc)
    return accs

# Training loop
for epoch in range(200):
    loss = train()
    train_acc, val_acc, test_acc = test()
    if epoch % 20 == 0:
        print(f'Epoch {epoch:03d}, Loss: {loss:.4f}, Train: {train_acc:.4f}, Val: {val_acc:.4f}, Test: {test_acc:.4f}')

# === STEP 6: Visualize Embeddings (Optional) ===
def plot_embedding(model, data):
    model.eval()
    x = model(data).detach().cpu().numpy()
    y = data.y.cpu().numpy()
    from sklearn.manifold import TSNE
    tsne = TSNE(n_components=2)
    x_tsne = tsne.fit_transform(x)

    plt.figure(figsize=(8,6))
    plt.scatter(x_tsne[:,0], x_tsne[:,1], c=y, cmap="jet", s=15)
    plt.title("GCN Node Embeddings (t-SNE)")
    plt.colorbar()
    plt.show()

plot_embedding(model, data)

# === STEP A: Synthetic Graph Fingerprint Creation ===
from torch_geometric.data import Data
import torch

def create_fingerprint_graph(num_nodes=10, feature_dim=dataset.num_node_features):
    # Create random edges (sparse graph)
    edge_index = torch.randint(0, num_nodes, (2, num_nodes), dtype=torch.long)

    # Random features for each node (uniform between 0 and 1)
    x = torch.rand((num_nodes, feature_dim), dtype=torch.float)

    # Dummy labels (not used for training)
    y = torch.zeros(num_nodes, dtype=torch.long)

    # Create a PyG Data object
    fingerprint = Data(x=x, edge_index=edge_index, y=y)
    return fingerprint

# Create 5 fingerprint graphs
fingerprints = [create_fingerprint_graph() for _ in range(5)]

# Put model in eval mode
model.eval()

# Move to same device
for fp in fingerprints:
    fp = fp.to(device)

# Collect model responses
fingerprint_outputs = []

for fp in fingerprints:
    with torch.no_grad():
        out = model(fp.to(device))
        fingerprint_outputs.append(out.cpu().numpy())

# Convert to array of response vectors
import numpy as np
fingerprint_outputs = np.array(fingerprint_outputs)
print("Fingerprint Output Shape:", fingerprint_outputs.shape)

# visualising fingerprint graphs
import networkx as nx
from torch_geometric.utils import to_networkx
import matplotlib.pyplot as plt

def visualize_fingerprint_graph(fingerprint, idx=0):
    G = to_networkx(fingerprint, to_undirected=True)
    plt.figure(figsize=(4, 4))
    nx.draw(G, with_labels=True, node_color='skyblue', edge_color='gray', node_size=500)
    plt.title(f"Fingerprint Graph #{idx}")
    plt.show()

# Visualize all 5 fingerprints
for i, fp in enumerate(fingerprints):
    visualize_fingerprint_graph(fp, idx=i+1)

# Simulate a stolen model trained using soft labels from original GCN
surrogate_model = GCN().to(device)
surrogate_optimizer = torch.optim.Adam(surrogate_model.parameters(), lr=0.01, weight_decay=5e-4)

# Use soft outputs from original model as targets
soft_targets = model(data).detach()

def train_surrogate():
    surrogate_model.train()
    surrogate_optimizer.zero_grad()
    out = surrogate_model(data)
    loss = F.kl_div(out[data.train_mask], soft_targets[data.train_mask], reduction='batchmean', log_target=True)
    loss.backward()
    surrogate_optimizer.step()
    return loss.item()

for epoch in range(200):
    loss = train_surrogate()
    if epoch % 50 == 0:
        print(f"[Surrogate] Epoch {epoch} - Loss: {loss:.4f}")

# Compare GCN and surrogate outputs on fingerprints
original_outputs = []
surrogate_outputs = []

model.eval()
surrogate_model.eval()

for fp in fingerprints:
    with torch.no_grad():
        original = model(fp.to(device)).cpu().numpy()
        surrogate = surrogate_model(fp.to(device)).cpu().numpy()
        original_outputs.append(original)
        surrogate_outputs.append(surrogate)

# Compute average L2 distances
l2_dists = [np.linalg.norm(orig - sur) for orig, sur in zip(original_outputs, surrogate_outputs)]
print("Average L2 distances between original and surrogate on fingerprint graphs:")
print(np.round(l2_dists, 4))

from sklearn.neural_network import MLPClassifier
from sklearn.metrics import classification_report

# Flatten each graph's output into a 1D vector
def flatten_outputs(output_list):
    return [out.flatten() for out in output_list]

X = flatten_outputs(original_outputs + surrogate_outputs)
y = [1] * len(original_outputs) + [0] * len(surrogate_outputs)  # 1 = real, 0 = stolen

# Train Univerifier
univerifier = MLPClassifier(hidden_layer_sizes=(64, 32), max_iter=500, random_state=42)
univerifier.fit(X, y)

# Eval
y_pred = univerifier.predict(X)
print("Univerifier Results:\n", classification_report(y, y_pred))

import seaborn as sns

# Plot for first fingerprint (real vs. surrogate)
fp_idx = 0
real_out = original_outputs[fp_idx].flatten()
sur_out = surrogate_outputs[fp_idx].flatten()

plt.figure(figsize=(10, 5))
sns.lineplot(data=real_out, label="Original GCN")
sns.lineplot(data=sur_out, label="Surrogate GCN")
plt.title("Output Vector of Fingerprint Graph #1")
plt.xlabel("Dimension Index")
plt.ylabel("Log-Probability")
plt.legend()
plt.show()

# Simulate a set of irrelevant GNNs (different init, trained from scratch)
irrelevant_models = []

for _ in range(5):  # Feel free to expand to 10–20
    model_ir = GCN().to(device)
    optimizer_ir = torch.optim.Adam(model_ir.parameters(), lr=0.01, weight_decay=5e-4)

    # Standard training
    for _ in range(100):
        model_ir.train()
        optimizer_ir.zero_grad()
        out = model_ir(data)
        loss = F.nll_loss(out[data.train_mask], data.y[data.train_mask])
        loss.backward()
        optimizer_ir.step()

    irrelevant_models.append(model_ir)

# Collect outputs on fingerprints from irrelevant models
irrelevant_outputs = []

for model_ir in irrelevant_models:
    model_ir.eval()
    model_outputs = []
    for fp in fingerprints:
        with torch.no_grad():
            out = model_ir(fp.to(device)).cpu().numpy()
            model_outputs.append(out)
    irrelevant_outputs.append(model_outputs)

# Flatten to same structure: list of np arrays
irrelevant_outputs_flat = [o.flatten() for graph_out in irrelevant_outputs for o in graph_out]

# Flatten all output vectors
X = flatten_outputs(original_outputs) + flatten_outputs(surrogate_outputs) + irrelevant_outputs_flat
y = [1]*len(original_outputs) + [0]*len(surrogate_outputs) + [-1]*len(irrelevant_outputs_flat)  # 1 = real, 0 = pirated, -1 = irrelevant

# Relabel to binary task: pirated vs. irrelevant (ownership check)
X_bin = flatten_outputs(surrogate_outputs + irrelevant_outputs_flat)
y_bin = [1]*len(surrogate_outputs) + [0]*len(irrelevant_outputs_flat)

from sklearn.metrics import roc_auc_score, confusion_matrix, ConfusionMatrixDisplay

# Retrain Univerifier
univerifier_bin = MLPClassifier(hidden_layer_sizes=(64, 32), max_iter=500, random_state=42)
univerifier_bin.fit(X_bin, y_bin)
y_pred_bin = univerifier_bin.predict(X_bin)

print("Updated Univerifier Results:\n", classification_report(y_bin, y_pred_bin))
ConfusionMatrixDisplay.from_predictions(y_bin, y_pred_bin)

import pandas as pd

# Save all outputs into one CSV file
df = pd.DataFrame({
    'ModelType': ['Real']*len(original_outputs) + ['Pirated']*len(surrogate_outputs) + ['Irrelevant']*len(irrelevant_outputs_flat),
    'L2Norm': l2_dists + [np.nan]*len(surrogate_outputs) + [np.nan]*len(irrelevant_outputs_flat),
    'OutputVec': [x.tolist() for x in X]
})

df.to_csv("gnnfingers_outputs.csv", index=False)
print("Saved outputs to 'gnnfingers_outputs.csv'")

#Building the univerifier model
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report
import numpy as np

class Univerifier(nn.Module):
    def __init__(self, input_dim):
        super(Univerifier, self).__init__()
        self.net = nn.Sequential(
            nn.Linear(input_dim, 128),
            nn.LeakyReLU(),
            nn.Linear(128, 64),
            nn.LeakyReLU(),
            nn.Linear(64, 32),
            nn.LeakyReLU(),
            nn.Linear(32, 2),  # Output: [pirated, irrelevant]
            nn.Softmax(dim=1)
        )

    def forward(self, x):
        return self.net(x)

def create_and_train_gcn(seed=0, fine_tune=False, base_model=None, data=None, epochs=100):
    torch.manual_seed(seed)
    model = GCN().to(device)

    if fine_tune and base_model is not None:
        model.load_state_dict(base_model.state_dict())  # Copy weights
        model.train()
    else:
        model.train()

    optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)

    for _ in range(epochs):
        optimizer.zero_grad()
        out = model(data)
        loss = F.nll_loss(out[data.train_mask], data.y[data.train_mask])
        loss.backward()
        optimizer.step()

    return model

# Train your base model first
base_model = GCN().to(device)
optimizer = torch.optim.Adam(base_model.parameters(), lr=0.01, weight_decay=5e-4)
for _ in range(200):  # Original full training
    base_model.train()
    optimizer.zero_grad()
    out = base_model(data)
    loss = F.nll_loss(out[data.train_mask], data.y[data.train_mask])
    loss.backward()
    optimizer.step()

# Generate pirated models (fine-tuned versions of base)
pirated_models = [create_and_train_gcn(seed=i, fine_tune=True, base_model=base_model, data=data, epochs=10) for i in range(20)]


# Generate irrelevant models (trained from scratch with different seeds)
irrelevant_models = [create_and_train_gcn(seed=100+i, fine_tune=False, base_model=None, data=data, epochs=100) for i in range(20)]

# Store output vectors
fingerprint_outputs_pos = []
fingerprint_outputs_neg = []

# Evaluate pirated (positive) models
for model in pirated_models:
    model.eval()
    outputs = []
    for fp in fingerprints:
        with torch.no_grad():
            fp = fp.to(device)
            out = model(fp)
            outputs.append(out.cpu().numpy())
    fingerprint_outputs_pos.append(np.concatenate(outputs))

# Evaluate irrelevant (negative) models
for model in irrelevant_models:
    model.eval()
    outputs = []
    for fp in fingerprints:
        with torch.no_grad():
            fp = fp.to(device)
            out = model(fp)
            outputs.append(out.cpu().numpy())
    fingerprint_outputs_neg.append(np.concatenate(outputs))

# Split into training and testing sets with stratification
from sklearn.utils import shuffle
X = np.array(X_pos + X_neg)
y = np.array([1]*len(X_pos) + [0]*len(X_neg))
X, y = shuffle(X, y, random_state=42)

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, stratify=y, random_state=42)

# Add this here to inspect class balance
unique, counts = np.unique(y_train, return_counts=True)
print("Train class distribution:", dict(zip(unique, counts)))
unique, counts = np.unique(y_test, return_counts=True)
print("Test class distribution:", dict(zip(unique, counts)))

# Convert to PyTorch tensors
X_train_tensor = torch.tensor(X_train, dtype=torch.float32)
y_train_tensor = torch.tensor(y_train, dtype=torch.long)
X_test_tensor = torch.tensor(X_test, dtype=torch.float32)
y_test_tensor = torch.tensor(y_test, dtype=torch.long)

# === Model evaluation ===
model.eval()
with torch.no_grad():
    test_outputs = model(X_test_tensor)
    pred_probs = test_outputs.detach().numpy()  # ← This is what threshold uses
    test_preds = np.argmax(pred_probs, axis=1)

# === Original classification report ===
from sklearn.metrics import classification_report, accuracy_score
print("\nTest Accuracy:", accuracy_score(y_test, test_preds))
print("\nClassification Report:\n", classification_report(y_test, test_preds, target_names=["Irrelevant", "Pirated"]))

# Add this block HERE to apply a custom threshold λ ===
lambda_thresh = 0.75  # You can experiment with 0.5, 0.6, ..., 0.9
pred_labels_thresh = (pred_probs[:, 1] > lambda_thresh).astype(int)

print(f"\n🔍 Predictions using threshold λ = {lambda_thresh}:")
for i, probs in enumerate(pred_probs):
    print(f"Sample {i+1}: Prob Pirated={probs[1]:.2f}, Predicted={pred_labels_thresh[i]}, True={y_test[i]}")

print("\nAccuracy with λ threshold:", accuracy_score(y_test, pred_labels_thresh))

import matplotlib.pyplot as plt

pred_probs = model(X_test_tensor).detach().numpy()
pred_labels = np.argmax(pred_probs, axis=1)

for i, probs in enumerate(pred_probs):
    print(f"Sample {i+1}: Prob Pirated={probs[1]:.2f}, Predicted Label={pred_labels[i]}, True Label={y_test[i]}")

#ARUC CURVE
import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix, auc

# Make sure pred_probs and y_test are already defined
lambdas = np.linspace(0, 1, 101)
robustness = []  # True Positive Rate (TPR) — pirated correctly detected
uniqueness = []  # True Negative Rate (TNR) — irrelevant correctly rejected

for λ in lambdas:
    preds = (pred_probs[:, 1] > λ).astype(int)

    # Confusion matrix format: [[TN, FP], [FN, TP]]
    cm = confusion_matrix(y_test, preds, labels=[0, 1])

    if cm.shape != (2, 2):
        cm = np.array([[0, 0], [0, 0]])  # Handle cases with missing classes

    tn, fp, fn, tp = cm.ravel()

    tpr = tp / (tp + fn) if (tp + fn) > 0 else 0  # Robustness
    tnr = tn / (tn + fp) if (tn + fp) > 0 else 0  # Uniqueness

    robustness.append(tpr)
    uniqueness.append(tnr)

plt.figure(figsize=(6, 5))
plt.plot(uniqueness, robustness, marker='o', label='GNNFingers')
plt.xlabel("Uniqueness (True Negative Rate)")
plt.ylabel("Robustness (True Positive Rate)")
plt.title("Robustness vs. Uniqueness Curve")
plt.grid(True)
plt.legend()

# Calculate Area under the curve (ARUC)
sorted_pairs = sorted(zip(uniqueness, robustness))
x_sorted, y_sorted = zip(*sorted_pairs)
area = auc(x_sorted, y_sorted)
print(f"\nARUC Score: {area:.4f}")

plt.show()

!pip install torch-geometric torch-scatter torch-sparse torch-cluster -f https://data.pyg.org/whl/torch-2.0.0+cpu.html

from torch_geometric.datasets import TUDataset
from torch_geometric.loader import DataLoader
from torch_geometric.nn import GINConv, global_add_pool
import torch.nn as nn
import torch.nn.functional as F
import torch

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
dataset = TUDataset(root='/tmp/ENZYMES', name='ENZYMES')
dataset = dataset.shuffle()

from torch_geometric.nn import GINConv, global_add_pool
import torch.nn.functional as F
import torch.nn as nn

class DeepGIN(nn.Module):
    def __init__(self, in_channels, hidden_channels, out_channels):
        super().__init__()
        self.conv1 = GINConv(nn.Sequential(nn.Linear(in_channels, hidden_channels),
                                           nn.ReLU(), nn.Linear(hidden_channels, hidden_channels)))
        self.bn1 = nn.BatchNorm1d(hidden_channels)
        self.conv2 = GINConv(nn.Sequential(nn.Linear(hidden_channels, hidden_channels),
                                           nn.ReLU(), nn.Linear(hidden_channels, hidden_channels)))
        self.bn2 = nn.BatchNorm1d(hidden_channels)
        self.conv3 = GINConv(nn.Sequential(nn.Linear(hidden_channels, hidden_channels),
                                           nn.ReLU(), nn.Linear(hidden_channels, hidden_channels)))
        self.bn3 = nn.BatchNorm1d(hidden_channels)
        self.fc = nn.Linear(hidden_channels, out_channels)

    def forward(self, x, edge_index, batch):
        x = F.relu(self.bn1(self.conv1(x, edge_index)))
        x = F.relu(self.bn2(self.conv2(x, edge_index)))
        x = F.relu(self.bn3(self.conv3(x, edge_index)))
        x = global_add_pool(x, batch)
        return F.log_softmax(self.fc(x), dim=-1)

# Split dataset
# Reduce batch size to improve stability on small datasets
train_loader = DataLoader(dataset[:500], batch_size=16, shuffle=True)
test_loader = DataLoader(dataset[500:], batch_size=16)

model = DeepGIN(dataset.num_features, 64, 6).to(device)

optimizer = torch.optim.Adam(model.parameters(), lr=0.005)  # Smaller LR

def train():
    model.train()
    total_loss = 0
    for data in train_loader:
        data = data.to(device)
        optimizer.zero_grad()
        out = model(data.x, data.edge_index, data.batch)
        loss = F.nll_loss(out, data.y)
        loss.backward()
        optimizer.step()
        total_loss += loss.item()
    return total_loss / len(train_loader)

def test(loader):
    model.eval()
    correct = 0
    for data in loader:
        data = data.to(device)
        out = model(data.x, data.edge_index, data.batch)
        pred = out.argmax(dim=1)
        correct += (pred == data.y).sum().item()
    return correct / len(loader.dataset)

for epoch in range(1, 201):
    loss = train()
    acc = test(test_loader)
    if epoch % 10 == 0:
        print(f'Epoch {epoch:03d}, Loss: {loss:.4f}, Test Acc: {acc:.4f}')

import torch
from torch_geometric.data import Data
import random

def generate_synthetic_graph(num_nodes=8, num_node_features=dataset.num_features, edge_prob=0.3):
    edge_index = []
    for i in range(num_nodes):
        for j in range(num_nodes):
            if i != j and random.random() < edge_prob:
                edge_index.append([i, j])
    if not edge_index:
        edge_index = [[0, 1], [1, 0]]  # minimal connectivity
    edge_index = torch.tensor(edge_index, dtype=torch.long).t().contiguous()

    x = torch.randn((num_nodes, num_node_features), dtype=torch.float)
    y = torch.tensor([0])  # dummy label
    return Data(x=x, edge_index=edge_index, y=y)

# Generate a set of fingerprints
num_fingerprints = 32
fingerprint_graphs = [generate_synthetic_graph() for _ in range(num_fingerprints)]

print(f"Generated {len(fingerprint_graphs)} fingerprint graphs.")

!pip install matplotlib networkx

import matplotlib.pyplot as plt
import networkx as nx
from torch_geometric.utils import to_networkx

def visualize_fingerprint_graphs(fingerprint_graphs, num_to_plot=6):
    plt.figure(figsize=(12, 8))
    for i in range(num_to_plot):
        g = to_networkx(fingerprint_graphs[i], to_undirected=True)
        plt.subplot(2, (num_to_plot + 1) // 2, i + 1)
        nx.draw(g, with_labels=True, node_color='skyblue', node_size=500, font_size=8)
        plt.title(f'Fingerprint #{i+1}')
    plt.tight_layout()
    plt.show()

#  Ploting first 6 fingerprint graphs
visualize_fingerprint_graphs(fingerprint_graphs, num_to_plot=6)

import torch
import torch.nn.functional as F
from torch_geometric.nn import GCNConv

class GCN(torch.nn.Module):
    def __init__(self, in_channels, hidden_channels, out_channels=2):
        super(GCN, self).__init__()
        self.conv1 = GCNConv(in_channels, hidden_channels)
        self.conv2 = GCNConv(hidden_channels, out_channels)

    def forward(self, x, edge_index):
        x = self.conv1(x, edge_index)
        x = F.relu(x)
        x = self.conv2(x, edge_index)
        return x

# Increase the number of pirated and irrelevant models
num_models = 50

pirated_models = [GCN(in_channels=dataset.num_features, hidden_channels=64).to(device) for _ in range(num_models)]
irrelevant_models = [GCN(in_channels=dataset.num_features, hidden_channels=64).to(device) for _ in range(num_models)]

# Make sure to train them here or reuse already-trained ones

# Fingerprint responses
def get_fingerprint_outputs(models, fingerprint_graphs):
    outputs = []
    for model_idx, model in enumerate(models):
        model.eval()
        fingerprint_probs = []
        print(f"\n--- Evaluating Model {model_idx + 1} ---")
        for g_idx, g in enumerate(fingerprint_graphs):
            g = g.to(device)

            # Manually strip any extra attributes
            g_dict = g.to_dict()
            x = g_dict['x']
            edge_index = g_dict['edge_index']

            try:
                with torch.no_grad():
                    out = model(x, edge_index)  # Force exactly 2 args
                    prob = out.softmax(dim=-1).mean(dim=0)
                    fingerprint_probs.append(prob.cpu().numpy())
            except Exception as e:
                print(f"Error on fingerprint {g_idx + 1}: {e}")
                raise e

        outputs.append(fingerprint_probs)
    return outputs

outputs_pos = get_fingerprint_outputs(pirated_models, fingerprint_graphs)
outputs_neg = get_fingerprint_outputs(irrelevant_models, fingerprint_graphs)

import numpy as np
# Each item in outputs_pos is a list of softmax outputs per fingerprint graph
# So we first convert to array, then flatten
X_pos = [np.array(model_outputs).flatten() for model_outputs in outputs_pos]
X_neg = [np.array(model_outputs).flatten() for model_outputs in outputs_neg]

# Combine into final dataset
X = np.array(X_pos + X_neg)
y = np.array([1]*len(X_pos) + [0]*len(X_neg))

from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, roc_auc_score

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

clf = LogisticRegression(max_iter=1000)
clf.fit(X_train, y_train)

test_preds = clf.predict(X_test)
pred_probs = clf.predict_proba(X_test)

print("Classification Report:\n", classification_report(y_test, test_preds, target_names=["Irrelevant", "Pirated"]))
print("ARUC Score:", roc_auc_score(y_test, pred_probs[:, 1]))

# Evaluate with custom threshold
lambda_thresh = 0.75  # You can experiment with 0.6, 0.7, 0.8
pred_labels_thresh = (pred_probs[:, 1] > lambda_thresh).astype(int)

print(f"\nThreshold λ = {lambda_thresh}:")
for i, probs in enumerate(pred_probs):
    print(f"Sample {i+1}: Prob Pirated={probs[1]:.2f}, Predicted={pred_labels_thresh[i]}, True={y_test[i]}")

import matplotlib.pyplot as plt
from sklearn.metrics import roc_curve, auc

# Get ROC curve data
fpr, tpr, thresholds = roc_curve(y_test, pred_probs[:, 1])
roc_auc = auc(fpr, tpr)

# Plot
plt.figure(figsize=(6, 5))
plt.plot(fpr, tpr, color='blue', lw=2, label=f"ARUC = {roc_auc:.2f}")
plt.plot([0, 1], [0, 1], color='gray', linestyle='--')
plt.xlim([-0.05, 1.05])
plt.ylim([-0.05, 1.05])
plt.xlabel("False Positive Rate (Uniqueness)")
plt.ylabel("True Positive Rate (Robustness)")
plt.title("Robustness-Uniqueness (ARUC) Curve")
plt.legend(loc="lower right")
plt.grid(True)
plt.show()

# Load Cora+ train GAE link predictor
!pip install torch-geometric torch-sparse torch-scatter torch-cluster -f https://data.pyg.org/whl/torch-2.0.0+cpu.html

import torch
import torch.nn.functional as F
from torch_geometric.nn import GCNConv, GAE
from torch_geometric.datasets import Planetoid
from torch_geometric.utils import train_test_split_edges

# Load Cora
dataset = Planetoid(root='/tmp/Cora', name='Cora')
data = dataset[0]
data.train_mask = data.val_mask = data.test_mask = None  # Disable masks
data = train_test_split_edges(data)  # Required for link prediction
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

from torch_geometric.nn import GCNConv

class GCNEncoder(torch.nn.Module):
    def __init__(self, in_channels, out_channels):
        super().__init__()
        self.conv1 = GCNConv(in_channels, 2 * out_channels)
        self.conv2 = GCNConv(2 * out_channels, out_channels)

    def forward(self, x, edge_index):
        x = F.relu(self.conv1(x, edge_index))
        return self.conv2(x, edge_index)

# Initialize model
channels = 32
model = GAE(GCNEncoder(dataset.num_features, channels)).to(device)
x, train_pos_edge_index = data.x.to(device), data.train_pos_edge_index.to(device)
optimizer = torch.optim.Adam(model.parameters(), lr=0.01)

# Training loop
def train():
    model.train()
    optimizer.zero_grad()
    z = model.encode(x, train_pos_edge_index)
    loss = model.recon_loss(z, train_pos_edge_index)
    loss.backward()
    optimizer.step()
    return loss.item()

# Run for 200 epochs
for epoch in range(1, 201):
    loss = train()
    if epoch % 20 == 0:
        print(f"Epoch {epoch:03d}, Loss: {loss:.4f}")

from torch_geometric.utils import negative_sampling
from sklearn.metrics import roc_auc_score, average_precision_score

@torch.no_grad()
def test():
    model.eval()
    z = model.encode(x, train_pos_edge_index)

    def get_scores(pos_edge_index, neg_edge_index):
        # Positive
        pos_scores = (z[pos_edge_index[0]] * z[pos_edge_index[1]]).sum(dim=-1)
        # Negative
        neg_scores = (z[neg_edge_index[0]] * z[neg_edge_index[1]]).sum(dim=-1)

        y_true = torch.cat([torch.ones(pos_scores.size(0)), torch.zeros(neg_scores.size(0))])
        y_pred = torch.cat([pos_scores, neg_scores])

        return roc_auc_score(y_true.cpu(), y_pred.cpu()), average_precision_score(y_true.cpu(), y_pred.cpu())

    # Use val/test edges
    val_auc, val_ap = get_scores(data.val_pos_edge_index.to(device), data.val_neg_edge_index.to(device))
    test_auc, test_ap = get_scores(data.test_pos_edge_index.to(device), data.test_neg_edge_index.to(device))

    return val_auc, val_ap, test_auc, test_ap

val_auc, val_ap, test_auc, test_ap = test()
print(f"Validation AUC: {val_auc:.4f}, AP: {val_ap:.4f}")
print(f"Test AUC: {test_auc:.4f}, AP: {test_ap:.4f}")

import networkx as nx
from torch_geometric.utils import from_networkx
import random

def generate_fingerprint_graphs_link_pred(num_graphs=32, num_nodes=20, edge_prob=0.2):
    graphs = []
    for i in range(num_graphs):
        G = nx.erdos_renyi_graph(n=num_nodes, p=edge_prob)
        while not nx.is_connected(G):
            G = nx.erdos_renyi_graph(n=num_nodes, p=edge_prob)

        for n in G.nodes():
            G.nodes[n]['x'] = torch.randn(dataset.num_features)  # Use same feature dim

        pyg_graph = from_networkx(G)
        pyg_graph.x = torch.stack([G.nodes[n]['x'] for n in G.nodes()])
        graphs.append(pyg_graph)

    return graphs

# ✅ Generate fingerprint graphs
fingerprint_graphs = generate_fingerprint_graphs_link_pred()
print(f"Generated {len(fingerprint_graphs)} fingerprint graphs.")

def train_gae_model(data, epochs=100, hidden_dim=32):
    model = GAE(Encoder(dataset.num_features, hidden_dim)).to(device)
    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)

    model.train()
    for epoch in range(epochs):
        optimizer.zero_grad()
        z = model.encode(data.x.to(device), data.train_pos_edge_index.to(device))
        loss = model.recon_loss(z, data.train_pos_edge_index.to(device))
        loss.backward()
        optimizer.step()

    return model

import torch.nn.functional as F
from torch_geometric.nn import GCNConv

class Encoder(torch.nn.Module):
    def __init__(self, in_channels, out_channels):
        super(Encoder, self).__init__()
        self.conv1 = GCNConv(in_channels, 2 * out_channels)
        self.conv2 = GCNConv(2 * out_channels, out_channels)

    def forward(self, x, edge_index):
        x = F.relu(self.conv1(x, edge_index))
        return self.conv2(x, edge_index)

pirated_models = []

for i in range(20):
    print(f"Training Pirated Model {i+1}/20")
    model = train_gae_model(data, epochs=100)
    pirated_models.append(model)

irrelevant_models = []

for i in range(20):
    print(f"Training Irrelevant Model {i+1}/20")

    # Clone the original data and shuffle edges
    data_shuffled = data.clone()
    edge_index = data.train_pos_edge_index
    num_edges = edge_index.size(1)
    shuffled = edge_index[:, torch.randperm(num_edges)]  # Shuffle column order

    data_shuffled.train_pos_edge_index = shuffled

    model = train_gae_model(data_shuffled, epochs=100)
    irrelevant_models.append(model)

def get_fingerprint_outputs(models, fingerprint_graphs):
    outputs = []

    for idx, model in enumerate(models):
        model.eval()
        model_outputs = []
        print(f"--- Evaluating Model {idx + 1} ---")
        for g in fingerprint_graphs:
            try:
                x = g.x.to(device)
                edge_index = g.edge_index.to(device)
                with torch.no_grad():
                    out = model.encode(x, edge_index)
                    model_outputs.append(out.mean(dim=0).cpu().numpy())
            except Exception as e:
                print(f"Error on fingerprint {len(model_outputs)+1}: {e}")
        outputs.append(np.stack(model_outputs))

    return outputs

outputs_pos = get_fingerprint_outputs(pirated_models, fingerprint_graphs)
outputs_neg = get_fingerprint_outputs(irrelevant_models, fingerprint_graphs)

from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report, roc_auc_score, average_precision_score
from sklearn.model_selection import train_test_split

# Flatten fingerprints
X_pos = [x.flatten() for x in outputs_pos]
X_neg = [x.flatten() for x in outputs_neg]

X = np.array(X_pos + X_neg)
y = np.array([1] * len(X_pos) + [0] * len(X_neg))  # 1 = pirated, 0 = irrelevant

# Split into train/test
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, stratify=y, random_state=42)

# Train logistic regression
clf = LogisticRegression(max_iter=1000)
clf.fit(X_train, y_train)

# Evaluate
test_preds = clf.predict(X_test)
pred_probs = clf.predict_proba(X_test)

print("\n📋 Classification Report:")
print(classification_report(y_test, test_preds, target_names=["Irrelevant", "Pirated"]))

# ARUC & AP
aruc = roc_auc_score(y_test, pred_probs[:, 1])
ap = average_precision_score(y_test, pred_probs[:, 1])
print(f"\n📈 ARUC Score: {aruc:.4f}, Average Precision: {ap:.4f}")

lambda_thresh = 0.75  # You can also try values like 0.6, 0.7, 0.8
pred_labels_thresh = (pred_probs[:, 1] > lambda_thresh).astype(int)

print(f"\n🔍 Threshold λ = {lambda_thresh}:")
for i, probs in enumerate(pred_probs):
    print(f"Sample {i+1}: Prob Pirated={probs[1]:.2f}, Predicted={pred_labels_thresh[i]}, True={y_test[i]}")

from torch_geometric.datasets import TUDataset

# Load GREC dataset
dataset = TUDataset(root='/tmp/PROTEINS', name='PROTEINS').shuffle()

print(dataset)

import torch.nn.functional as F
from torch_geometric.nn import GCNConv, global_mean_pool
from torch_geometric.loader import DataLoader

class GCNGraphClassifier(torch.nn.Module):
    def __init__(self, in_channels, hidden_channels, out_channels):
        super().__init__()
        self.conv1 = GCNConv(in_channels, hidden_channels)
        self.conv2 = GCNConv(hidden_channels, hidden_channels)
        self.lin = torch.nn.Linear(hidden_channels, out_channels)

    def forward(self, x, edge_index, batch):
        x = F.relu(self.conv1(x, edge_index))
        x = F.relu(self.conv2(x, edge_index))
        x = global_mean_pool(x, batch)
        return self.lin(x)

torch.manual_seed(42)
train_loader = DataLoader(dataset[:900], batch_size=32, shuffle=True)
test_loader = DataLoader(dataset[900:], batch_size=32)

model = GCNGraphClassifier(dataset.num_features, 64, dataset.num_classes).to(device)
optimizer = torch.optim.Adam(model.parameters(), lr=0.01)

def train():
    model.train()
    for data in train_loader:
        data = data.to(device)
        optimizer.zero_grad()
        out = model(data.x, data.edge_index, data.batch)
        loss = F.cross_entropy(out, data.y)
        loss.backward()
        optimizer.step()

def test(loader):
    model.eval()
    correct = 0
    for data in loader:
        data = data.to(device)
        out = model(data.x, data.edge_index, data.batch)
        pred = out.argmax(dim=1)
        correct += int((pred == data.y).sum())
    return correct / len(loader.dataset)

for epoch in range(1, 201):
    train()
    if epoch % 20 == 0:
        acc = test(test_loader)
        print(f"Epoch {epoch:03d}, Test Accuracy: {acc:.4f}")

from torch_geometric.data import Data
import random

def generate_fingerprint_graphs(num_graphs=32, num_nodes=10):
    graphs = []
    for _ in range(num_graphs):
        edge_index = torch.randint(0, num_nodes, (2, 2 * num_nodes))
        x = torch.rand((num_nodes, dataset.num_features))  # same feature dimension as PROTEINS
        graphs.append(Data(x=x, edge_index=edge_index))
    return graphs

fingerprint_graphs = generate_fingerprint_graphs()
print(f"✅ Generated {len(fingerprint_graphs)} fingerprint graphs.")

import copy
import torch
import random

def perturb_fingerprint_graphs(graphs, num_perturbations=5, noise_level=0.05):
    """
    Generate perturbed versions of each fingerprint graph.
    Each graph will be copied `num_perturbations` times with noise added to node features.
    """
    all_perturbed = []
    for g in graphs:
        for _ in range(num_perturbations):
            new_g = copy.deepcopy(g)
            noise = noise_level * torch.randn_like(new_g.x)
            new_g.x = new_g.x + noise
            all_perturbed.append(new_g)
    return all_perturbed

class Encoder(torch.nn.Module):
    def __init__(self, in_channels, out_channels):
        super().__init__()
        self.conv1 = GCNConv(in_channels, 2 * out_channels)
        self.conv2 = GCNConv(2 * out_channels, out_channels)

    def forward(self, x, edge_index):
        x = self.conv1(x, edge_index).relu()
        return self.conv2(x, edge_index)

import torch
import torch.nn.functional as F
from torch_geometric.loader import DataLoader
import torch_geometric.nn as geom_nn

# Define global hidden dimension for graph classification
hidden_dim = 64

# Define the GCN-based graph classifier (for PROTEINS)
class GCNGraphClassifier(torch.nn.Module):
    def __init__(self, in_channels, hidden_channels, out_channels):
        super().__init__()
        # Use two linear layers with ReLU; aggregate with global mean pooling
        self.lin1 = torch.nn.Linear(in_channels, hidden_channels)
        self.lin2 = torch.nn.Linear(hidden_channels, hidden_channels)
        self.lin3 = torch.nn.Linear(hidden_channels, out_channels)

    def forward(self, x, edge_index, batch):
        x = F.relu(self.lin1(x))
        x = F.relu(self.lin2(x))
        x = geom_nn.global_mean_pool(x, batch)
        return self.lin3(x)

# Function to train a GCN classifier on PROTEINS (graph classification)
def train_gcn_graph_classifier(shuffle_labels=False):
    model = GCNGraphClassifier(dataset.num_features, hidden_dim, dataset.num_classes).to(device)
    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)

    train_loader = DataLoader(dataset[:900], batch_size=32, shuffle=True)
    test_loader = DataLoader(dataset[900:], batch_size=32)

    for epoch in range(1, 101):
        model.train()
        for data in train_loader:
            data = data.to(device)
            if shuffle_labels:
                data.y = data.y[torch.randperm(data.y.size(0))]  # Shuffle labels
            optimizer.zero_grad()
            out = model(data.x, data.edge_index, data.batch)
            loss = F.cross_entropy(out, data.y)
            loss.backward()
            optimizer.step()

    return model

# Train 10 pirated and 10 irrelevant graph classifiers
pirated_models = []
irrelevant_models = []

for _ in range(10):
    pirated_models.append(train_gcn_graph_classifier(shuffle_labels=False))
    irrelevant_models.append(train_gcn_graph_classifier(shuffle_labels=True))

print("✅ Pirated and irrelevant graph classifiers trained.")

from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report, roc_auc_score, average_precision_score
from sklearn.model_selection import train_test_split

# Split into train/test (40% test, stratified)
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.4, stratify=y, random_state=42
)

# Train logistic regression
clf = LogisticRegression(max_iter=2000)
clf.fit(X_train, y_train)

# Predict & evaluate
y_pred = clf.predict(X_test)
y_prob = clf.predict_proba(X_test)[:, 1]

print("\n📋 Classification Report:")
print(classification_report(y_test, y_pred, target_names=["Irrelevant", "Pirated"]))

aruc = roc_auc_score(y_test, y_prob)
ap = average_precision_score(y_test, y_prob)
print(f"\n📈 ARUC Score: {aruc:.4f}, Average Precision: {ap:.4f}")

import numpy as np
import torch
import random
from torch_geometric.data import Data

def generate_fingerprints(num_graphs, num_nodes=10):
    graphs = []
    for _ in range(num_graphs):
        edge_index = torch.randint(0, num_nodes, (2, 2 * num_nodes))
        x = torch.rand((num_nodes, dataset.num_features))
        graphs.append(Data(x=x, edge_index=edge_index))
    return graphs

def get_graph_outputs(models, graphs):
    """Runs each model on each fingerprint graph; returns list of [num_graphs×num_classes] arrays."""
    all_outputs = []
    for model in models:
        model.eval()
        outputs = []
        for g in graphs:
            g = g.to(device)
            with torch.no_grad():
                out = model(g.x, g.edge_index, torch.zeros(g.num_nodes, dtype=torch.long).to(device))
                outputs.append(out.cpu().numpy())
        all_outputs.append(np.stack(outputs))
    return all_outputs

from sklearn.linear_model import LogisticRegression
from sklearn.metrics import roc_auc_score
from sklearn.model_selection import train_test_split

def evaluate_with_fingerprints(k):
    # 1. Generate k fingerprint graphs
    fps = generate_fingerprints(k)

    # 2. Collect outputs for pirated & irrelevant models
    outputs_pos_k = get_graph_outputs(pirated_models, fps)
    outputs_neg_k = get_graph_outputs(irrelevant_models, fps)

    # 3. Flatten and label
    X_pos = [arr.flatten() for arr in outputs_pos_k]
    X_neg = [arr.flatten() for arr in outputs_neg_k]
    X = np.vstack(X_pos + X_neg)
    y = np.array([1] * len(X_pos) + [0] * len(X_neg))

    # 4. Train/test split
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.4, stratify=y, random_state=42
    )

    # 5. Train Univiser (logistic regression) and compute ARUC
    clf = LogisticRegression(max_iter=2000)
    clf.fit(X_train, y_train)
    y_prob = clf.predict_proba(X_test)[:, 1]
    aruc = roc_auc_score(y_test, y_prob)
    return aruc

# Run ablation for k = 16, 32, 64
for k in [16, 32, 64]:
    aruc_k = evaluate_with_fingerprints(k)
    print(f"Fingerprints={k:2d} → ARUC = {aruc_k:.4f}")

import numpy as np
from torch_geometric.data import Data

def generate_ablation_fingerprints(k, num_nodes=10, mode="both", noise_level=0.05):
    """
    Create k fingerprint graphs, each with:
      - mode="both"    → random edges + node‐feature noise
      - mode="feat"    → fixed random edges + node‐feature noise only
      - mode="adj"     → random edges only (features zero‐mean)
    """
    fingerprints = []
    # First, fix a random base adjacency if we need a consistent graph for “feat” mode:
    if mode in ("feat",):
        base_edge_index = torch.randint(0, num_nodes, (2, 2 * num_nodes))
    for _ in range(k):
        if mode == "both":
            # Completely random graph + noisy features
            edge_index = torch.randint(0, num_nodes, (2, 2 * num_nodes))
            x = torch.rand((num_nodes, dataset.num_features)) + noise_level * torch.randn((num_nodes, dataset.num_features))
        elif mode == "feat":
            # Use the same base adjacency but only add feature noise each time
            edge_index = base_edge_index.clone()
            x = torch.rand((num_nodes, dataset.num_features)) + noise_level * torch.randn((num_nodes, dataset.num_features))
        elif mode == "adj":
            # Random adjacency each time but keep features fixed (e.g. random once)
            edge_index = torch.randint(0, num_nodes, (2, 2 * num_nodes))
            x = torch.rand((num_nodes, dataset.num_features))  # no extra noise
        else:
            raise ValueError("mode must be 'both', 'feat', or 'adj'")

        fur = Data(x=x, edge_index=edge_index)
        fingerprints.append(fur)
    return fingerprints

# Evaluate ARUC for each ablation mode at k=32 fingerprints
from sklearn.metrics import roc_auc_score
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split

def compute_aruc_for_mode(mode):
    fps = generate_ablation_fingerprints(32, mode=mode)
    out_pos = get_graph_outputs(pirated_models, fps)
    out_neg = get_graph_outputs(irrelevant_models, fps)

    X_pos = [arr.flatten() for arr in out_pos]
    X_neg = [arr.flatten() for arr in out_neg]
    X = np.vstack(X_pos + X_neg)
    y = np.array([1]*len(X_pos) + [0]*len(X_neg))

    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, stratify=y, random_state=42)
    clf = LogisticRegression(max_iter=2000).fit(X_train, y_train)
    y_prob = clf.predict_proba(X_test)[:, 1]
    return roc_auc_score(y_test, y_prob)

for mode in ["both", "feat", "adj"]:
    aruc = compute_aruc_for_mode(mode)
    print(f"Mode={mode:4s} → ARUC = {aruc:.4f}")

import torch
import torch.nn.functional as F
from torch_geometric.datasets import TUDataset
from torch_geometric.loader import DataLoader
import torch_geometric.nn as geom_nn
from torch_geometric.data import Data
import numpy as np
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import roc_auc_score
from sklearn.model_selection import train_test_split

# ─── 1. Load PROTEINS dataset and set device ─────────────────────────────────
dataset = TUDataset(root='/tmp/PROTEINS', name='PROTEINS').shuffle()
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

# ─── 2. Define GCN-based graph classifier for PROTEINS ─────────────────────────
hidden_dim = 64

class GCNGraphClassifier(torch.nn.Module):
    def __init__(self, in_channels, hidden_channels, out_channels):
        super().__init__()
        self.lin1 = torch.nn.Linear(in_channels, hidden_channels)
        self.lin2 = torch.nn.Linear(hidden_channels, hidden_channels)
        self.lin3 = torch.nn.Linear(hidden_channels, out_channels)

    def forward(self, x, edge_index, batch):
        x = F.relu(self.lin1(x))
        x = F.relu(self.lin2(x))
        x = geom_nn.global_mean_pool(x, batch)
        return self.lin3(x)

# ─── 3. Training function for GCN classifier ────────────────────────────────────
def train_gcn_graph_classifier(shuffle_labels=False):
    model = GCNGraphClassifier(dataset.num_features, hidden_dim, dataset.num_classes).to(device)
    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)
    train_loader = DataLoader(dataset[:900], batch_size=32, shuffle=True)

    for epoch in range(1, 101):
        model.train()
        for batch in train_loader:
            batch = batch.to(device)
            if shuffle_labels:
                batch.y = batch.y[torch.randperm(batch.y.size(0))]
            optimizer.zero_grad()
            out = model(batch.x, batch.edge_index, batch.batch)
            loss = F.cross_entropy(out, batch.y)
            loss.backward()
            optimizer.step()

    return model

# ─── 4. Train 50 pirated and 50 irrelevant models ───────────────────────────────
pirated_models = []
irrelevant_models = []
for _ in range(50):
    pirated_models.append(train_gcn_graph_classifier(shuffle_labels=False))
    irrelevant_models.append(train_gcn_graph_classifier(shuffle_labels=True))

# ─── 5. Initialize 32 fingerprint graphs (adj_vars + feat_vars) ────────────────
num_fp = 32
num_nodes = 10
feature_dim = dataset.num_features

# Adjacency variables (continuous, to be optimized)
adj_vars = [torch.rand((num_nodes, num_nodes), requires_grad=True, device=device) for _ in range(num_fp)]
# Feature variables (continuous, to be optimized)
feat_vars = [torch.rand((num_nodes, feature_dim), requires_grad=True, device=device) for _ in range(num_fp)]

# ─── 6. Define the Univerifier network ─────────────────────────────────────────
class Univerifier(torch.nn.Module):
    def __init__(self, input_dim):
        super().__init__()
        self.lin1 = torch.nn.Linear(input_dim, 128)
        self.lin2 = torch.nn.Linear(128, 64)
        self.lin3 = torch.nn.Linear(64, 2)  # → [P(ir), P(not ir)]

    def forward(self, x):
        x = F.leaky_relu(self.lin1(x))
        x = F.leaky_relu(self.lin2(x))
        return F.softmax(self.lin3(x), dim=-1)

input_dim = num_fp * dataset.num_classes  # 32 × 2 = 64
U = Univerifier(input_dim=input_dim).to(device)
optimizer_U = torch.optim.Adam(U.parameters(), lr=1e-3)

# ─── 7. Helper to build a PyG Data object from adj_vars[i], feat_vars[i] ───────
def build_pyg_graph(idx):
    """Construct a Data object using adj_vars[idx] (threshold at 0.5) and feat_vars[idx]."""
    A_logits = adj_vars[idx]  # shape [num_nodes, num_nodes]
    edge_index = (A_logits > 0.5).nonzero().t().contiguous().to(device)
    x_noisy = feat_vars[idx]
    # batch vector: all nodes in one graph → batch=zeros
    batch_vec = torch.zeros(num_nodes, dtype=torch.long, device=device)
    return x_noisy, edge_index, batch_vec

# ─── 8. collect_outputs: get each model’s fingerprint output features ──────────
def collect_outputs(models_list):
    """
    For each model Fi in models_list,
    run Fi on all 32 fingerprint graphs → get logits → average over nodes →
    collect a [num_fp × num_classes] feature vector (flattened).
    Return a tensor of shape [len(models_list), num_fp × num_classes].
    """
    outs = []
    for Fi in models_list:
        Fi.eval()
        out_vals = []
        for idx in range(num_fp):
            x_noisy, edge_index, batch_vec = build_pyg_graph(idx)
            with torch.no_grad():
                logits = Fi(x_noisy.to(device), edge_index, batch_vec)
                prob = F.softmax(logits, dim=-1).mean(dim=0)  # shape [num_classes]
                out_vals.append(prob)
        # Concatenate 32 × [num_classes] → length = num_fp*num_classes
        outs.append(torch.cat(out_vals))
    return torch.stack(outs)  # shape [num_models, 64]

# ─── 9. Joint‐optimization loop to learn optimal feature fingerprints ──────────
epochs_joint = 200

for epoch in range(1, epochs_joint + 1):
    # 3a: Train Univerifier U with frozen feat_vars
    for var in feat_vars:
        var.requires_grad_(False)
    for p in U.parameters():
        p.requires_grad_(True)

    optimizer_U.zero_grad()
    pos_feats = collect_outputs(pirated_models, allow_grad_features=False)    # (50,64)
    neg_feats = collect_outputs(irrelevant_models, allow_grad_features=False) # (50,64)
    all_feats = torch.cat([pos_feats, neg_feats], dim=0)                      # (100,64)
    labels    = torch.cat([torch.ones(50), torch.zeros(50)], dim=0).long().to(device)

    U_preds = U(all_feats)  # shape (100,2)
    loss_U = F.cross_entropy(U_preds, labels)
    loss_U.backward()
    optimizer_U.step()

    # 3b: Train only feat_vars (freeze U and Fi)
    for var in feat_vars:
        var.requires_grad_(True)
    for p in U.parameters():
        p.requires_grad_(False)

    optimizer_vars = torch.optim.Adam(feat_vars, lr=1e-2)
    optimizer_vars.zero_grad()

    pos_feats = collect_outputs(pirated_models, allow_grad_features=True)
    neg_feats = collect_outputs(irrelevant_models, allow_grad_features=True)
    all_feats = torch.cat([pos_feats, neg_feats], dim=0)

    U_preds = U(all_feats)
    loss_vars = F.cross_entropy(U_preds, labels)
    loss_vars.backward()
    optimizer_vars.step()

    if epoch % 20 == 0 or epoch == 1:
        print(f"Epoch {epoch:03d}, Univiser Loss: {loss_U.item():.4f}")

# ─── 10. Ablation on “both”, “feat”, “adj” using the *optimized* feat_vars ────────
def generate_ablation_fingerprints(k, mode="both", noise_level=0.05):
    """
    From the optimized feat_vars & fixed adj_vars, generate k fingerprint graphs.
    mode="both": use threshold(adj_vars[idx]) + feat_vars[idx].
    mode="feat": use threshold(adj_vars[idx]) + (feat_vars[idx] + small noise)
    mode="adj": use (threshold(adj_vars[idx]) with 5% random edge drops) + feat_vars[idx].
    """
    graphs = []
    for idx in range(k):
        A_logits = adj_vars[idx]
        if mode == "both":
            edge_index = (A_logits > 0.5).nonzero().t().contiguous().to(device)
            x = feat_vars[idx]
        elif mode == "feat":
            edge_index = (A_logits > 0.5).nonzero().t().contiguous().to(device)
            x = feat_vars[idx] + noise_level * torch.randn_like(feat_vars[idx])
        elif mode == "adj":
            thresh_mask = (A_logits > 0.5).float()
            drop_mask = (torch.rand_like(A_logits) > 0.05).float()
            A_new = A_logits * thresh_mask * drop_mask
            edge_index = (A_new > 0.5).nonzero().t().contiguous().to(device)
            x = feat_vars[idx]
        else:
            raise ValueError("mode must be 'both', 'feat', or 'adj'")
        graphs.append(Data(x=x, edge_index=edge_index))
    return graphs

def get_graph_outputs(models, graphs):
    """
    Collect (num_graphs × num_classes) outputs from each model, returned as a list of arrays.
    """
    all_outputs = []
    for model in models:
        model.eval()
        outputs = []
        for g in graphs:
            g = g.to(device)
            with torch.no_grad():
                out = model(g.x, g.edge_index, torch.zeros(g.num_nodes, dtype=torch.long).to(device))
                outputs.append(out.cpu().numpy())
        all_outputs.append(np.stack(outputs))
    return all_outputs

def compute_aruc_for_mode(mode):
    fps = generate_ablation_fingerprints(num_fp, mode=mode)
    outputs_pos = get_graph_outputs(pirated_models, fps)
    outputs_neg = get_graph_outputs(irrelevant_models, fps)

    X_pos = [arr.flatten() for arr in outputs_pos]
    X_neg = [arr.flatten() for arr in outputs_neg]
    X = np.vstack(X_pos + X_neg)
    y = np.array([1] * len(X_pos) + [0] * len(X_neg))

    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.4, stratify=y, random_state=42
    )
    clf = LogisticRegression(max_iter=2000).fit(X_train, y_train)
    y_prob = clf.predict_proba(X_test)[:, 1]
    return roc_auc_score(y_test, y_prob)

for mode in ["both", "feat", "adj"]:
    aruc = compute_aruc_for_mode(mode)
    print(f"Mode={mode:4s} → ARUC = {aruc:.4f}")

num_perturbations = 5
perturbed_fingerprints = perturb_fingerprint_graphs(fingerprint_graphs, num_perturbations=num_perturbations)
print(f"Generated {len(perturbed_fingerprints)} perturbed fingerprint graphs.")

from torch.nn.functional import cosine_similarity

def get_model_fingerprint_outputs(models, graphs):
    """
    Run all models on all fingerprint graphs and collect outputs.
    Returns: List[List[Tensor]] — outer list over models, inner list over graph outputs
    """
    all_outputs = []
    for model_idx, model in enumerate(models):
        print(f"Evaluating Model {model_idx + 1}")
        model.eval()
        outputs = []
        for g in graphs:
            g = g.to(device)
            try:
                out = model(g.x, g.edge_index)
            except:
                batch = torch.zeros(g.num_nodes, dtype=torch.long).to(device)
                out = model(g.x, g.edge_index, batch)
            outputs.append(out.flatten().detach().cpu())
        all_outputs.append(outputs)
    return all_outputs