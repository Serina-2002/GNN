# -*- coding: utf-8 -*-
"""LINUX

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1r4a9A4-SdQ4godDrCaKoQnTrQRmcsnwK
"""

# Install PyTorch for CPU
!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu
# Install PyTorch for CUDA (if runtime is GPU)
!pip install torch torchvision torchaudio
# Install torch-geometric and its dependencies
!pip install torch-scatter torch-sparse torch-cluster torch-spline-conv -f https://data.pyg.org/whl/torch-2.2.0+cpu.html
!pip install torch-geometric
# Additional utilities for visualization and data handling
!pip install scikit-learn matplotlib networkx

# Remove existing potentially broken installs
!pip uninstall -y torch-scatter torch-sparse torch-cluster torch-spline-conv torch-geometric

# Reinstall with specific compatible versions (works with torch 2.0+ CPU on Colab)
!pip install torch==2.0.1 torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu
!pip install torch-scatter torch-sparse torch-cluster torch-spline-conv -f https://data.pyg.org/whl/torch-2.0.0+cpu.html
!pip install torch-geometric

from google.colab import drive
drive.mount('/content/drive')

!mkdir -p /content/drive/MyDrive/gnnfingers_data/LINUX/raw

from google.colab import drive
drive.mount('/content/drive')

import os

print("üìÇ Files found:")
print(os.listdir('/content/drive/MyDrive/gnnfingers_data/LINUX/raw'))

file_path = '/content/drive/MyDrive/gnnfingers_data/LINUX/raw/fgraph_G.txt'

with open(file_path, 'r') as f:
    lines = f.readlines()

# Print the first 15 lines to inspect structure
print("üîç Preview of fgraph_G.txt:")
for line in lines[:15]:
    print(line.strip())

from torch_geometric.data import Data
import torch

file_path = '/content/drive/MyDrive/gnnfingers_data/LINUX/raw/fgraph_G.txt'

# Read edges
edges = []
nodes = set()
with open(file_path, 'r') as f:
    for line in f:
        src, dst = line.strip().split()
        edges.append((src, dst))
        nodes.update([src, dst])

# Map file names to node indices
node_list = sorted(list(nodes))
node_idx = {name: i for i, name in enumerate(node_list)}

# Convert to edge index
edge_index = torch.tensor(
    [[node_idx[src], node_idx[dst]] for src, dst in edges],
    dtype=torch.long
).t().contiguous()

# Simple features: identity matrix (one-hot encoding)
x = torch.eye(len(node_list), dtype=torch.float)

# Dummy label (optional)
y = torch.tensor([0])  # placeholder label

# Create PyG graph
data = Data(x=x, edge_index=edge_index, y=y)

print("‚úÖ Graph created")
print(f"Nodes: {data.num_nodes}, Edges: {data.num_edges}")

from torch_geometric.data import InMemoryDataset

class LinuxSingleGraphDataset(InMemoryDataset):
    def __init__(self, transform=None):
        super().__init__('.', transform)
        self.data, self.slices = self.collate([data])

    def get(self, idx):
        return self.data

dataset = LinuxSingleGraphDataset()
print(f"‚úÖ Dataset ready: {len(dataset)} graph(s)")

import torch.nn as nn
import torch.nn.functional as F
from torch_geometric.nn import GCNConv, global_mean_pool

class GCNMean(nn.Module):
    def __init__(self, in_channels, hidden_channels, out_channels):
        super().__init__()
        self.conv1 = GCNConv(in_channels, hidden_channels)
        self.conv2 = GCNConv(hidden_channels, hidden_channels)
        self.lin = nn.Linear(hidden_channels, out_channels)

    def forward(self, x, edge_index, batch):
        x = F.relu(self.conv1(x, edge_index))
        x = F.dropout(x, p=0.5, training=self.training)
        x = F.relu(self.conv2(x, edge_index))
        x = global_mean_pool(x, batch)
        return self.lin(x)

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

# Load the single graph from the dataset
graph = dataset[0].to(device)

# Simulate batch (all nodes belong to one graph)
graph.batch = torch.zeros(graph.num_nodes, dtype=torch.long).to(device)

# Create model
model = GCNMean(in_channels=graph.num_node_features, hidden_channels=64, out_channels=2).to(device)

optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)

def train():
    model.train()
    optimizer.zero_grad()
    out = model(graph.x, graph.edge_index, graph.batch)
    loss = F.cross_entropy(out, torch.tensor([0, 1], dtype=torch.long).to(device))  # Dummy labels
    loss.backward()
    optimizer.step()
    return loss.item()

import torch
num_nodes = len(node_list)

# Use small dense random features instead of 5275-dim identity matrix
x = torch.randn(num_nodes, 32)  # e.g., 32 features per node

import random

# Sample only 100k edges to reduce load (instead of 14M)
sampled_edges = random.sample(edges, 100000)

# Convert to edge_index
edge_index = torch.tensor(
    [[node_idx[src], node_idx[dst]] for src, dst in sampled_edges],
    dtype=torch.long
).t().contiguous()

device = torch.device('cpu')

# Step 1: Load edges
edges = []
nodes = set()
with open('/content/drive/MyDrive/gnnfingers_data/LINUX/raw/fgraph_G.txt', 'r') as f:
    for line in f:
        src, dst = line.strip().split()
        edges.append((src, dst))
        nodes.update([src, dst])

# Step 2: Build smaller graph with random features
node_list = sorted(list(nodes))
node_idx = {name: i for i, name in enumerate(node_list)}
num_nodes = len(node_list)

# Sample 100K edges to avoid crash
import random
sampled_edges = random.sample(edges, min(100000, len(edges)))

edge_index = torch.tensor(
    [[node_idx[src], node_idx[dst]] for src, dst in sampled_edges],
    dtype=torch.long
).t().contiguous()

x = torch.randn(num_nodes, 32)
y = torch.tensor([0])

from torch_geometric.data import Data
data = Data(x=x, edge_index=edge_index, y=y)

print(f"‚úÖ Nodes: {data.num_nodes}, Edges: {data.num_edges}, Features: {data.num_node_features}")

import torch.nn as nn
import torch.nn.functional as F
from torch_geometric.nn import GCNConv, global_mean_pool

class GCNMean(nn.Module):
    def __init__(self, in_channels, hidden_channels, out_channels):
        super().__init__()
        self.conv1 = GCNConv(in_channels, hidden_channels)
        self.conv2 = GCNConv(hidden_channels, hidden_channels)
        self.lin = nn.Linear(hidden_channels, out_channels)

    def forward(self, x, edge_index, batch):
        x = F.relu(self.conv1(x, edge_index))
        x = F.dropout(x, p=0.5, training=self.training)
        x = F.relu(self.conv2(x, edge_index))
        x = global_mean_pool(x, batch)
        return self.lin(x)

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

data = data.to(device)
data.batch = torch.zeros(data.num_nodes, dtype=torch.long).to(device)

model = GCNMean(in_channels=32, hidden_channels=64, out_channels=2).to(device)
optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)

def train():
    model.train()
    optimizer.zero_grad()
    out = model(data.x, data.edge_index, data.batch)
    loss = F.cross_entropy(out, torch.tensor([0], dtype=torch.long).to(device))

    loss.backward()
    optimizer.step()
    return loss.item()

for epoch in range(1, 101):
    loss = train()
    if epoch % 10 == 0 or epoch == 1:
        print(f"Epoch {epoch:03d}, Loss: {loss:.4f}")

class FingerprintNetMLP(nn.Module):
    def __init__(self, input_dim=2, hidden_dim=16, fp_dim=7):
        super().__init__()
        self.mlp = nn.Sequential(
            nn.Linear(input_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, fp_dim)
        )

    def forward(self, graph_embedding):
        return self.mlp(graph_embedding)

# Use trained GCNMean to generate graph embedding
model.eval()
with torch.no_grad():
    graph_embedding = model(data.x, data.edge_index, data.batch)

# Create the fingerprint extractor
U = FingerprintNetMLP().to(device)

# Get the fingerprint
fingerprint = U(graph_embedding)
print(" Fingerprint vector (tensor):", fingerprint)

import copy

def generate_ablation_variant(data, mode='A_only', drop_ratio=0.1):
    """
    Generate a variant graph by modifying either attributes, structure, or both.
    - data: original PyG Data object
    - mode: 'A_only', 'S_only', 'A+S'
    - drop_ratio: fraction of nodes or edges/features to perturb
    """
    variant = copy.deepcopy(data)

    if mode in ['A_only', 'A+S']:
        # Perturb node features
        noise = torch.randn_like(variant.x) * 0.1
        variant.x += noise

    if mode in ['S_only', 'A+S']:
        # Randomly drop edges
        num_edges = variant.edge_index.size(1)
        keep_count = int((1 - drop_ratio) * num_edges)
        perm = torch.randperm(num_edges)[:keep_count]
        variant.edge_index = variant.edge_index[:, perm]

    return variant

# Create 100 variants for each attack type
num_variants = 100
variant_modes = ['A_only', 'S_only', 'A+S']
variants = {mode: [] for mode in variant_modes}

for mode in variant_modes:
    print(f"üîÑ Generating variants for {mode}...")
    for _ in range(num_variants):
        variant = generate_ablation_variant(data, mode=mode, drop_ratio=0.1)
        variants[mode].append(variant)

print("‚úÖ All graph variants created.")

import torch.nn.functional as F
from torch.nn.functional import cosine_similarity

def extract_fp(model, U_net, graph):
    model.eval()
    U_net.eval()
    with torch.no_grad():
        batch = torch.zeros(graph.num_nodes, dtype=torch.long).to(device)
        out = model(graph.x.to(device), graph.edge_index.to(device), batch)
        fp = U_net(out)
    return fp

# Original fingerprint
fp_orig = extract_fp(model, U, data)

# Extract for all variants
variant_fps = {mode: [] for mode in variants}

for mode in variants:
    print(f"üîç Extracting fingerprints for {mode} variants...")
    for g in variants[mode]:
        fp = extract_fp(model, U, g)
        variant_fps[mode].append(fp)

import numpy as np

def compute_similarities(fp_list, fp_orig):
    sims = []
    for fp in fp_list:
        sim = cosine_similarity(fp, fp_orig).item()
        sims.append(sim)
    return sims

similarities = {mode: compute_similarities(variant_fps[mode], fp_orig)
                for mode in variant_fps}

# Print average cosine similarities
for mode in similarities:
    sims = similarities[mode]
    print(f"{mode} - Avg Cosine Similarity: {np.mean(sims):.4f}")

# Original graph is class 1 (genuine)
train_graphs = [data] + variants["A_only"][:50] + variants["S_only"][:50] + variants["A+S"][:50]
labels = [1] + [1]*150  # all same model

# Create 150 fake graphs from permuted features (fake class 0)
import random
fake_graphs = [generate_ablation_variant(data, mode='A+S', drop_ratio=0.7) for _ in range(150)]
train_graphs += fake_graphs
labels += [0]*150

class FingerprintClassifier(nn.Module):
    def __init__(self, fp_dim=7):
        super().__init__()
        self.fc = nn.Linear(fp_dim, 2)  # Binary classifier

    def forward(self, fp):
        return self.fc(fp)

U = FingerprintNetMLP().to(device)
C = FingerprintClassifier().to(device)
optimizer_U = torch.optim.Adam(list(U.parameters()) + list(C.parameters()), lr=0.001)

for epoch in range(1, 51):
    total_loss = 0
    for g, label in zip(train_graphs, labels):
        optimizer_U.zero_grad()
        batch = torch.zeros(g.num_nodes, dtype=torch.long).to(device)
        emb = model(g.x.to(device), g.edge_index.to(device), batch)
        fp = U(emb)
        logits = C(fp)
        target = torch.tensor([label], dtype=torch.long).to(device)
        loss = F.cross_entropy(logits, target)
        loss.backward()
        optimizer_U.step()
        total_loss += loss.item()
    if epoch % 10 == 0 or epoch == 1:
        print(f"Epoch {epoch}, Loss: {total_loss:.4f}")

fp_orig = extract_fp(model, U, data)
variant_fps = {mode: [] for mode in variants}

for mode in variants:
    print(f"üîç Re-extracting fingerprints for {mode}...")
    for g in variants[mode]:
        fp = extract_fp(model, U, g)
        variant_fps[mode].append(fp)

similarities = {mode: compute_similarities(variant_fps[mode], fp_orig)
                for mode in variant_fps}

for mode in similarities:
    sims = similarities[mode]
    print(f"{mode} - Avg Cosine Similarity: {np.mean(sims):.4f}")

similarities = {mode: compute_similarities(variant_fps[mode], fp_orig)
                for mode in variant_fps}

for mode in similarities:
    sims = similarities[mode]
    print(f"{mode} - Avg Cosine Similarity: {np.mean(sims):.4f}")

train_graphs = [data]                     # original
labels = [1]                              # genuine

# Treat A_only + S_only + A+S as pirated
for mode in ['A_only', 'S_only', 'A+S']:
    train_graphs.extend(variants[mode][:50])
    labels.extend([0] * 50)

for mode in similarities:
    sims = similarities[mode]
    print(f"{mode} - Avg Cosine Similarity: {np.mean(sims):.4f}")

variants = {
    mode: [generate_ablation_variant(data, mode=mode, drop_ratio=0.3) for _ in range(100)]
    for mode in ["A_only", "S_only", "A+S"]
}

import matplotlib.pyplot as plt

for mode, sims in similarities.items():
    plt.figure()
    plt.hist(sims, bins=30, alpha=0.7)
    plt.title(f"Cosine Similarity Histogram: {mode}")
    plt.xlabel("Cosine Similarity with Original Fingerprint")
    plt.ylabel("Frequency")
    plt.grid(True)
    plt.show()

file_path = '/content/drive/MyDrive/gnnfingers_data/LINUX/raw/fgraph_C.txt'

with open(file_path, 'r') as f:
    for i in range(15):
        print(f.readline().strip())

from torch_geometric.data import Data

def load_graph_from_txt(file_path, feature_dim=32):
    edges = []
    nodes = set()

    with open(file_path, 'r') as f:
        for line in f:
            src, dst = line.strip().split()
            edges.append((src, dst))
            nodes.update([src, dst])

    node_list = sorted(list(nodes))
    node_idx = {name: i for i, name in enumerate(node_list)}

    edge_index = torch.tensor(
        [[node_idx[src], node_idx[dst]] for src, dst in edges],
        dtype=torch.long
    ).t().contiguous()

    x = torch.randn(len(node_list), feature_dim)  # dense random features
    y = torch.tensor([0])  # dummy label

    return Data(x=x, edge_index=edge_index, y=y)

# Load fgraph_C.txt as a PyG graph
graph_C = load_graph_from_txt('/content/drive/MyDrive/gnnfingers_data/LINUX/raw/fgraph_C.txt')
graph_C.batch = torch.zeros(graph_C.num_nodes, dtype=torch.long)

fp_C = extract_fp(model, U, graph_C)
cos_sim = cosine_similarity(fp_C, fp_orig).item()
print(f"üîç Cosine similarity with original fingerprint: {cos_sim:.4f}")

import os


irrelevant_files = [
    'fgraph_C.txt',
    'fgraph_G.txt',
    'fgraph_F_D.txt',
    'fgraph_I_all.txt'
]

base_path = '/content/drive/MyDrive/gnnfingers_data/LINUX/raw/'
irrelevant_sims = {}

for fname in irrelevant_files:
    path = os.path.join(base_path, fname)
    try:
        g = load_graph_from_txt(path)
        g.batch = torch.zeros(g.num_nodes, dtype=torch.long)
        fp = extract_fp(model, U, g)
        sim = cosine_similarity(fp, fp_orig).item()
        irrelevant_sims[fname] = sim
        print(f"{fname}: Cosine similarity = {sim:.4f}")
    except Exception as e:
        print(f"Error processing {fname}: {e}")