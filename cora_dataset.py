# -*- coding: utf-8 -*-
"""Cora dataset

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1dFN4G78oJiZFgfrvW3wFRceoLAaJsor7
"""

# Install PyTorch Geometric and dependencies (make sure torch is already installed)
!pip install torch-scatter torch-sparse torch-cluster torch-spline-conv torch-geometric -f https://data.pyg.org/whl/torch-2.0.0+cpu.html
!pip install torch-scatter torch-sparse torch-cluster torch-spline-conv torch-geometric -f https://data.pyg.org/whl/torch-2.0.0+cu118.html

import torch
import torch.nn.functional as F
from torch_geometric.datasets import Planetoid
from torch_geometric.nn import GCNConv, SAGEConv
from torch_geometric.loader import DataLoader

# Load Cora dataset
dataset = Planetoid(root='data/Cora', name='Cora')
data = dataset[0].to('cuda' if torch.cuda.is_available() else 'cpu')
device = data.x.device

# Define GCNMean model
class GCNMean(torch.nn.Module):
    def __init__(self, in_channels, hidden_channels, out_channels):
        super().__init__()
        self.conv1 = GCNConv(in_channels, hidden_channels)
        self.conv2 = GCNConv(hidden_channels, out_channels)
        self.dropout = 0.5

    def forward(self, x, edge_index):
        x = F.relu(self.conv1(x, edge_index))
        x = F.dropout(x, p=self.dropout, training=self.training)
        x = self.conv2(x, edge_index)
        return x

# Instantiate and train model
model = GCNMean(dataset.num_node_features, 64, dataset.num_classes).to(device)
optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)

for epoch in range(200):
    model.train()
    optimizer.zero_grad()
    out = model(data.x, data.edge_index)
    loss = F.cross_entropy(out[data.train_mask], data.y[data.train_mask])
    loss.backward()
    optimizer.step()

    # Optional: print accuracy every 20 epochs
    if epoch % 20 == 0:
        model.eval()
        pred = out.argmax(dim=1)
        acc = (pred[data.val_mask] == data.y[data.val_mask]).float().mean().item()
        print(f"Epoch {epoch:03d} - Val Acc: {acc:.4f}")

# Save the victim model
import os
os.makedirs("checkpoints", exist_ok=True)

# Save the victim model
torch.save(model.state_dict(), "checkpoints/victim_Cora_GCNMean.pth")
print("‚úÖ Saved victim GCNMean for Cora.")

from torch_geometric.loader import DataLoader

def extract_fingerprint(model, data=None):
    model.eval()
    if data is None:
        data = dataset[0].to(device)
    with torch.no_grad():
        x = F.relu(model.conv1(data.x, data.edge_index))
        x = F.dropout(x, p=model.dropout, training=False)
        x = model.conv2(x, data.edge_index)
        fingerprint = x.mean(dim=0).unsqueeze(0)  # global mean pooling
    return fingerprint

fp_victim = extract_fingerprint(model)
print("‚úÖ Extracted fingerprint from victim GCNMean")

from tqdm import tqdm
import os

def generate_fingerprint_variants(model_class, variant_name, dataset_name, num_variants=200, base_dir="variants"):
    os.makedirs(f"{base_dir}/{dataset_name}_{variant_name}/positive", exist_ok=True)
    os.makedirs(f"{base_dir}/{dataset_name}_{variant_name}/negative", exist_ok=True)

    # Positive variants (model with same architecture and dataset)
    print("Generating positive variants‚Ä¶")
    for i in tqdm(range(num_variants), desc="positive"):
        m = model_class(dataset.num_node_features, 64, dataset.num_classes).to(device)
        train(m)  # same training loop
        torch.save(m.state_dict(), f"{base_dir}/{dataset_name}_{variant_name}/positive/positive_{i:03d}.pth")

    # Negative variants (model with same architecture but on shuffled labels)
    print("Generating negative variants‚Ä¶")
    for i in tqdm(range(num_variants), desc="negative"):
        m = model_class(dataset.num_node_features, 64, dataset.num_classes).to(device)
        train(m, shuffle_labels=True)  # define shuffle inside train()
        torch.save(m.state_dict(), f"{base_dir}/{dataset_name}_{variant_name}/negative/negative_{i:03d}.pth")

    print(f"‚úÖ Saved {2*num_variants} variants to {base_dir}/{dataset_name}_{variant_name}/")

def train(model, epochs=200, shuffle_labels=False):
    model.train()
    optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)

    # If negative variant, shuffle the labels
    y_train = data.y.clone()
    if shuffle_labels:
        y_train = y_train[torch.randperm(y_train.size(0))]

    for epoch in range(epochs):
        optimizer.zero_grad()
        out = model(data.x, data.edge_index)
        loss = F.cross_entropy(out[data.train_mask], y_train[data.train_mask])
        loss.backward()
        optimizer.step()

generate_fingerprint_variants(
    model_class=GCNMean,
    variant_name="GCNMean",
    dataset_name="Cora",
    num_variants=200,
    base_dir="variants"
)

import glob
from tqdm import tqdm

def build_fingerprint_dataset(variant_path_pattern, label, model_class):
    fingerprints = []
    labels = []
    model = model_class(dataset.num_node_features, 64, dataset.num_classes).to(device)

    for pth_file in tqdm(glob.glob(variant_path_pattern)):
        model.load_state_dict(torch.load(pth_file, map_location=device))
        model.eval()
        fp = extract_fingerprint(model).cpu()  # [1, d]
        fingerprints.append(fp)
        labels.append(label)

    return torch.cat(fingerprints, dim=0), torch.tensor(labels)

# Extract fingerprints from variant models
fp_pos, y_pos = build_fingerprint_dataset("variants/Cora_GCNMean/positive/*.pth", 1, GCNMean)
fp_neg, y_neg = build_fingerprint_dataset("variants/Cora_GCNMean/negative/*.pth", 0, GCNMean)

# Combine all fingerprints
X_all = torch.cat([fp_pos, fp_neg], dim=0)
y_all = torch.cat([y_pos, y_neg], dim=0)

print("‚úÖ All fingerprints and labels collected.")

import glob
from tqdm import tqdm

def build_fingerprint_dataset(variant_path_pattern, label, model_class):
    fingerprints = []
    labels = []
    model = model_class(dataset.num_node_features, 64, dataset.num_classes).to(device)

    for pth_file in tqdm(glob.glob(variant_path_pattern)):
        model.load_state_dict(torch.load(pth_file, map_location=device))
        model.eval()
        fp = extract_fingerprint(model).cpu()  # shape [1, d]
        fingerprints.append(fp)
        labels.append(label)

    return torch.cat(fingerprints, dim=0), torch.tensor(labels)

# Positive (pirated) and Negative (irrelevant) fingerprints
fp_pos, y_pos = build_fingerprint_dataset("variants/Cora_GCNMean/positive/*.pth", 1, GCNMean)
fp_neg, y_neg = build_fingerprint_dataset("variants/Cora_GCNMean/negative/*.pth", 0, GCNMean)

# Combine into one dataset
X_all = torch.cat([fp_pos, fp_neg], dim=0)
y_all = torch.cat([y_pos, y_neg], dim=0)

print(f"‚úÖ Fingerprints shape: {X_all.shape}, Labels shape: {y_all.shape}")

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X_all, y_all, test_size=0.2, stratify=y_all, random_state=42)
print(f"‚úÖ Train set: {X_train.shape}, Test set: {X_test.shape}")

import torch.nn as nn

class Univerifier(nn.Module):
    def __init__(self, input_dim):
        super().__init__()
        self.model = nn.Sequential(
            nn.Linear(input_dim, 128),
            nn.LeakyReLU(),
            nn.Linear(128, 64),
            nn.LeakyReLU(),
            nn.Linear(64, 2)  # Output logits for [irrelevant, pirated]
        )

    def forward(self, x):
        return self.model(x)

from torch.utils.data import TensorDataset, DataLoader

# Prepare data
train_dataset = TensorDataset(X_train, y_train)
test_dataset = TensorDataset(X_test, y_test)
train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=32)

# Instantiate model
model_U = Univerifier(input_dim=X_train.shape[1]).to(device)
criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(model_U.parameters(), lr=0.001)

# Train loop
for epoch in range(20):
    model_U.train()
    total_loss = 0
    for xb, yb in train_loader:
        xb, yb = xb.to(device), yb.to(device)
        optimizer.zero_grad()
        out = model_U(xb)
        loss = criterion(out, yb)
        loss.backward()
        optimizer.step()
        total_loss += loss.item()
    print(f"Epoch {epoch+1:02d} - Loss: {total_loss:.4f}")

# Evaluation function
def evaluate(model, dataloader):
    model.eval()
    total = 0
    correct = 0
    tp = 0  # true positives (pirated correctly detected)
    tn = 0  # true negatives (irrelevant correctly rejected)
    total_pos = 0
    total_neg = 0

    with torch.no_grad():
        for xb, yb in dataloader:
            xb, yb = xb.to(device), yb.to(device)
            logits = model(xb)
            preds = logits.argmax(dim=1)
            correct += (preds == yb).sum().item()
            total += yb.size(0)
            tp += ((preds == 1) & (yb == 1)).sum().item()
            tn += ((preds == 0) & (yb == 0)).sum().item()
            total_pos += (yb == 1).sum().item()
            total_neg += (yb == 0).sum().item()

    acc = correct / total
    robustness = tp / total_pos
    uniqueness = tn / total_neg

    return acc, robustness, uniqueness

# Evaluate
acc, rob, uniq = evaluate(model_U, test_loader)
print(f"‚úÖ Accuracy: {acc:.4f}")
print(f"üîê Robustness (TPR): {rob:.4f}")
print(f"üõ°Ô∏è Uniqueness (TNR): {uniq:.4f}")

import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics import auc

# Get softmax scores on test set
model_U.eval()
all_scores = []
all_labels = []

with torch.no_grad():
    for xb, yb in test_loader:
        xb = xb.to(device)
        logits = model_U(xb)
        probs = F.softmax(logits, dim=1)[:, 1].cpu()  # score for class 1 (pirated)
        all_scores.extend(probs.numpy())
        all_labels.extend(yb.numpy())

all_scores = np.array(all_scores)
all_labels = np.array(all_labels)

# Sweep lambda thresholds
lambdas = np.linspace(0, 1, 100)
robustness_list = []
uniqueness_list = []

for lam in lambdas:
    preds = (all_scores > lam).astype(int)
    tp = np.sum((preds == 1) & (all_labels == 1))
    tn = np.sum((preds == 0) & (all_labels == 0))
    total_pos = np.sum(all_labels == 1)
    total_neg = np.sum(all_labels == 0)

    robustness = tp / total_pos if total_pos else 0
    uniqueness = tn / total_neg if total_neg else 0

    robustness_list.append(robustness)
    uniqueness_list.append(uniqueness)

# Compute ARUC (area under robustness-uniqueness curve)
aruc_score = auc(uniqueness_list, robustness_list)

# Plotting
plt.figure(figsize=(6,5))
plt.plot(uniqueness_list, robustness_list, label=f"ARUC = {aruc_score:.3f}")
plt.xlabel("Uniqueness (TNR)")
plt.ylabel("Robustness (TPR)")
plt.title("Robustness-Uniqueness Curve (GNNFingers on Cora/GCN)")
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()

from torch_geometric.nn import SAGEConv

class SAGEMean(torch.nn.Module):
    def __init__(self, in_channels, hidden_channels, out_channels):
        super().__init__()
        self.conv1 = SAGEConv(in_channels, hidden_channels)
        self.conv2 = SAGEConv(hidden_channels, out_channels)
        self.dropout = 0.5

    def forward(self, x, edge_index):
        x = F.relu(self.conv1(x, edge_index))
        x = F.dropout(x, p=self.dropout, training=self.training)
        x = self.conv2(x, edge_index)
        return x

# Instantiate model
model_sage = SAGEMean(dataset.num_node_features, 64, dataset.num_classes).to(device)
optimizer = torch.optim.Adam(model_sage.parameters(), lr=0.01, weight_decay=5e-4)

# Training loop
for epoch in range(200):
    model_sage.train()
    optimizer.zero_grad()
    out = model_sage(data.x, data.edge_index)
    loss = F.cross_entropy(out[data.train_mask], data.y[data.train_mask])
    loss.backward()
    optimizer.step()

    if epoch % 20 == 0:
        model_sage.eval()
        pred = out.argmax(dim=1)
        acc = (pred[data.val_mask] == data.y[data.val_mask]).float().mean().item()
        print(f"Epoch {epoch:03d} - Val Acc: {acc:.4f}")

# Save the trained victim model
torch.save(model_sage.state_dict(), "checkpoints/victim_Cora_SAGEMean.pth")
print("‚úÖ Saved victim GraphSAGE for Cora.")

fp_sage = extract_fingerprint(model_sage)
print("‚úÖ Extracted fingerprint from victim SAGEMean")

generate_fingerprint_variants(
    model_class=SAGEMean,
    variant_name="SAGEMean",
    dataset_name="Cora",
    num_variants=200,
    base_dir="variants"
)

import os
import torch
from torch_geometric.data import Data

def load_variants(variant_dir):
    graphs = []
    for fname in sorted(os.listdir(variant_dir)):
        if fname.endswith(".pt"):
            path = os.path.join(variant_dir, fname)
            g = torch.load(path)
            graphs.append(g)
    return graphs

def extract_fp(victim_model, U_net, graphs, batch_size=32, device='cuda' if torch.cuda.is_available() else 'cpu'):
    from torch.nn import functional as F
    from torch_geometric.loader import DataLoader
    from torch_geometric.nn import global_mean_pool

    victim_model.eval()
    U_net.eval()

    fingerprints = []

    loader = DataLoader(graphs, batch_size=batch_size, shuffle=False)
    for batch in loader:
        batch = batch.to(device)
        x = F.relu(victim_model.conv1(batch.x, batch.edge_index))
        x = F.dropout(x, p=victim_model.dropout, training=False)
        x = victim_model.conv2(x, batch.edge_index)
        pooled = global_mean_pool(x, batch.batch)
        out = U_net(pooled)
        fingerprints.append(out.detach().cpu())

    return torch.cat(fingerprints, dim=0)

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch_geometric.nn import SAGEConv

class SAGEMean(nn.Module):
    def __init__(self, in_channels=1433, hidden_channels=128, out_channels=128, dropout=0.5):
        super(SAGEMean, self).__init__()
        self.conv1 = SAGEConv(in_channels, hidden_channels, aggr='mean')
        self.conv2 = SAGEConv(hidden_channels, out_channels, aggr='mean')
        self.dropout = dropout

    def forward(self, x, edge_index, batch):
        x = F.relu(self.conv1(x, edge_index))
        x = F.dropout(x, p=self.dropout, training=self.training)
        x = self.conv2(x, edge_index)
        return x

class FingerprintNetMLP(nn.Module):
    def __init__(self, in_dim=128, hidden_dim=64, out_dim=64):
        super(FingerprintNetMLP, self).__init__()
        self.mlp = nn.Sequential(
            nn.Linear(in_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, out_dim)
        )

    def forward(self, x):
        return self.mlp(x)

import torch.nn as nn

class FingerprintNetMLP(nn.Module):
    def __init__(self, in_dim=128, hidden_dim=64, out_dim=2):  # out_dim=2 for classification
        super(FingerprintNetMLP, self).__init__()
        self.mlp = nn.Sequential(
            nn.Linear(in_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, out_dim)
        )

    def forward(self, x):
        return self.mlp(x)

from google.colab import drive
drive.mount('/content/drive')

print(os.listdir("/content/drive/MyDrive"))

print(os.listdir("/content/drive/MyDrive/gnnfingers"))

import os

base_path = "/content/drive/MyDrive/gnnfingers/variants"
print("üìÅ Subfolders under variants/:")
print(os.listdir(base_path))

# Mount Google Drive
from google.colab import drive
drive.mount('/content/drive')

# --- Imports ---
import os
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch_geometric.datasets import Planetoid
from torch_geometric.nn import SAGEConv
from torch_geometric.loader import DataLoader

# --- Model (smaller) ---
class SAGEMean(nn.Module):
    def __init__(self, in_channels=1433, hidden_channels=64, out_channels=64, dropout=0.5):
        super(SAGEMean, self).__init__()
        self.conv1 = SAGEConv(in_channels, hidden_channels, aggr='mean')
        self.conv2 = SAGEConv(hidden_channels, out_channels, aggr='mean')
        self.dropout = dropout

    def forward(self, x, edge_index, batch=None):
        x = F.relu(self.conv1(x, edge_index))
        x = F.dropout(x, p=self.dropout, training=self.training)
        x = self.conv2(x, edge_index)
        return x

# --- Load Cora dataset ---
dataset = Planetoid(root='/tmp/Cora', name='Cora')
data = dataset[0].to('cuda' if torch.cuda.is_available() else 'cpu')

# --- Train victim (fewer epochs) ---
device = data.x.device
victim = SAGEMean().to(device)
optimizer = torch.optim.Adam(victim.parameters(), lr=0.01, weight_decay=5e-4)

victim.train()
for epoch in range(1, 21):  # ‚Üì reduced epochs
    optimizer.zero_grad()
    out = victim(data.x, data.edge_index)
    loss = F.cross_entropy(out[data.train_mask], data.y[data.train_mask])
    loss.backward()
    optimizer.step()
    if epoch % 5 == 0:
        print(f"Epoch {epoch:02d} - Loss: {loss.item():.4f}")

# Save victim model
os.makedirs("/content/drive/MyDrive/gnnfingers/victims", exist_ok=True)
torch.save(victim.state_dict(), "/content/drive/MyDrive/gnnfingers/victims/victim_Cora_SAGEMean.pth")
print("‚úÖ Victim model saved to Drive")

# --- Variant Generation ---
def generate_positive_variant(graph, seed):
    torch.manual_seed(seed)
    return graph.clone().to('cpu').clone().detach().apply_(lambda x: x)  # fast copy

def generate_negative_variant(graph, seed):
    torch.manual_seed(seed)
    g = graph.clone().to('cpu')
    g.y = torch.randint(0, graph.y.max().item() + 1, g.y.shape)
    return g

variant_dir = "/content/drive/MyDrive/gnnfingers/variants/Cora_SAGEMean"
os.makedirs(variant_dir, exist_ok=True)

for i in range(200):
    p_graph = generate_positive_variant(data.cpu(), i)
    n_graph = generate_negative_variant(data.cpu(), 1000 + i)
    torch.save(p_graph, f"{variant_dir}/P_{i}.pt")
    torch.save(n_graph, f"{variant_dir}/N_{i}.pt")
    if (i + 1) % 50 == 0:
        print(f"‚úÖ Saved {i+1}/200 variants")

print("‚úÖ All 400 variants saved to:", variant_dir)

import os
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch_geometric.loader import DataLoader
from torch_geometric.nn import SAGEConv, global_mean_pool

# Define victim model
class SAGEMean(nn.Module):
    def __init__(self, in_channels=1433, hidden_channels=64, out_channels=64, dropout=0.5):
        super(SAGEMean, self).__init__()
        self.conv1 = SAGEConv(in_channels, hidden_channels, aggr='mean')
        self.conv2 = SAGEConv(hidden_channels, out_channels, aggr='mean')
        self.dropout = dropout

    def forward(self, x, edge_index, batch=None):
        x = F.relu(self.conv1(x, edge_index))
        x = F.dropout(x, p=self.dropout, training=self.training)
        x = self.conv2(x, edge_index)
        return x

# Define fingerprint network
class FingerprintNetMLP(nn.Module):
    def __init__(self, in_dim=64, hidden_dim=32, out_dim=64):
        super(FingerprintNetMLP, self).__init__()
        self.mlp = nn.Sequential(
            nn.Linear(in_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, out_dim)
        )
    def forward(self, x):
        return self.mlp(x)

# Load variant graphs
def load_variants(variant_dir):
    graphs = []
    for fname in sorted(os.listdir(variant_dir)):
        if fname.endswith(".pt"):
            path = os.path.join(variant_dir, fname)
            g = torch.load(path, weights_only=False)  # üîß FIXED LINE
            graphs.append(g)
    return graphs

# Extract fingerprints
def extract_fp(victim_model, U_net, graphs, batch_size=32, device='cuda' if torch.cuda.is_available() else 'cpu'):
    victim_model.eval()
    U_net.eval()
    fingerprints = []
    loader = DataLoader(graphs, batch_size=batch_size, shuffle=False)
    for batch in loader:
        batch = batch.to(device)
        x = F.relu(victim_model.conv1(batch.x, batch.edge_index))
        x = F.dropout(x, p=victim_model.dropout, training=False)
        x = victim_model.conv2(x, batch.edge_index)
        pooled = global_mean_pool(x, batch.batch)
        out = U_net(pooled)
        fingerprints.append(out.detach().cpu())
    return torch.cat(fingerprints, dim=0)

# Load model and data
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
victim = SAGEMean().to(device)
victim.load_state_dict(torch.load("/content/drive/MyDrive/gnnfingers/victims/victim_Cora_SAGEMean.pth"))
graphs = load_variants("/content/drive/MyDrive/gnnfingers/variants/Cora_SAGEMean")

# Extract fingerprints
fingerprints = extract_fp(victim, FingerprintNetMLP().to(device), graphs)

# Save fingerprints
os.makedirs("/content/drive/MyDrive/gnnfingers/fingerprints", exist_ok=True)
torch.save(fingerprints, "/content/drive/MyDrive/gnnfingers/fingerprints/fps_Cora_SAGEMean.pt")
print("‚úÖ Saved fingerprint matrix to /content/drive/MyDrive/gnnfingers/fingerprints/fps_Cora_SAGEMean.pt")

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import TensorDataset, DataLoader

# Step 1: Load fingerprints from Drive
fps = torch.load("/content/drive/MyDrive/gnnfingers/fingerprints/fps_Cora_SAGEMean.pt")

# Step 2: Create labels: 1 for positive, 0 for negative
labels = torch.cat([torch.ones(200), torch.zeros(200)]).long()

# Step 3: Train/test split
train_fps, test_fps = fps[:320], fps[320:]
train_labels, test_labels = labels[:320], labels[320:]

# Step 4: Define simple MLP classifier
class FingerprintNetMLP(nn.Module):
    def __init__(self, in_dim=64, hidden_dim=128, out_dim=64):
        super(FingerprintNetMLP, self).__init__()
        self.mlp = nn.Sequential(
            nn.Linear(in_dim, hidden_dim),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, out_dim)
        )

    def forward(self, x):
        return self.mlp(x)


# DataLoader setup
train_ds = TensorDataset(train_fps, train_labels)
test_ds = TensorDataset(test_fps, test_labels)
train_loader = DataLoader(train_ds, batch_size=32, shuffle=True)
test_loader = DataLoader(test_ds, batch_size=32)

# Step 5: Train the model
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
U = FingerprintNetMLP().to(device)
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(U.parameters(), lr=1e-3)

U.train()
for epoch in range(1, 11):
    total_loss = 0
    for x_batch, y_batch in train_loader:
        x_batch, y_batch = x_batch.to(device), y_batch.to(device)
        optimizer.zero_grad()
        logits = U(x_batch)
        loss = criterion(logits, y_batch)
        loss.backward()
        optimizer.step()
        total_loss += loss.item()
    print(f"Epoch {epoch:02d} - Loss: {total_loss:.4f}")

# Step 6: Evaluate on test set
U.eval()
correct = 0
total = 0
with torch.no_grad():
    for x_batch, y_batch in test_loader:
        x_batch, y_batch = x_batch.to(device), y_batch.to(device)
        preds = U(x_batch).argmax(dim=1)
        correct += (preds == y_batch).sum().item()
        total += y_batch.size(0)

acc = correct / total
print(f"‚úÖ Test Accuracy: {acc:.4f}")

# Step 7: Save Univerifier model to Drive
torch.save(U.state_dict(), "/content/drive/MyDrive/gnnfingers/checkpoints/U_Cora_SAGEMean.pth")
print("‚úÖ Univerifier saved to Drive at checkpoints/U_Cora_SAGEMean.pth")

print(fps.shape)       # Should be (400, 64)
print(labels[:10])     # Should be [1, 1, 1, ...] for first 200
print(labels[200:210]) # Should be [0, 0, 0, ...]

with torch.no_grad():
    test_probs = torch.softmax(U(test_fps.to(device)), dim=1)
    print(test_probs[:5])  # Check if outputs are all close to [0.5, 0.5]

# Mount Google Drive (if not already)
from google.colab import drive
drive.mount('/content/drive')

# Imports
import os
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch_geometric.datasets import Planetoid
from torch_geometric.nn import SAGEConv

# Define victim model
class SAGEMean(nn.Module):
    def __init__(self, in_channels=1433, hidden_channels=64, out_channels=64, dropout=0.5):
        super(SAGEMean, self).__init__()
        self.conv1 = SAGEConv(in_channels, hidden_channels, aggr='mean')
        self.conv2 = SAGEConv(hidden_channels, out_channels, aggr='mean')
        self.dropout = dropout

    def forward(self, x, edge_index, batch=None):
        x = F.relu(self.conv1(x, edge_index))
        x = F.dropout(x, p=self.dropout, training=self.training)
        x = self.conv2(x, edge_index)
        return x

# Load Cora dataset
from torch_geometric.datasets import Planetoid
dataset = Planetoid(root='/tmp/Cora', name='Cora')
data = dataset[0].to('cuda' if torch.cuda.is_available() else 'cpu')

# Initialize victim model
device = data.x.device
victim = SAGEMean().to(device)
optimizer = torch.optim.Adam(victim.parameters(), lr=0.01, weight_decay=5e-4)

# Train victim model
for epoch in range(1, 51):  # Train for 50 epochs
    victim.train()
    optimizer.zero_grad()
    out = victim(data.x, data.edge_index)
    loss = F.cross_entropy(out[data.train_mask], data.y[data.train_mask])
    loss.backward()
    optimizer.step()
    if epoch % 10 == 0:
        print(f"Epoch {epoch:02d} - Loss: {loss.item():.4f}")

# Save model to Drive
save_path = "/content/drive/MyDrive/gnnfingers/victims/victim_Cora_SAGEMean.pth"
os.makedirs(os.path.dirname(save_path), exist_ok=True)
torch.save(victim.state_dict(), save_path)
print(f"‚úÖ Retrained victim model saved to: {save_path}")

# === Mount Drive first ===
from google.colab import drive
drive.mount('/content/drive', force_remount=True)

# === Imports ===
import os
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch_geometric.loader import DataLoader
from torch_geometric.nn import SAGEConv, global_mean_pool

# === Victim Model Definition ===
class SAGEMean(nn.Module):
    def __init__(self, in_channels=1433, hidden_channels=64, out_channels=64, dropout=0.5):
        super(SAGEMean, self).__init__()
        self.conv1 = SAGEConv(in_channels, hidden_channels, aggr='mean')
        self.conv2 = SAGEConv(hidden_channels, out_channels, aggr='mean')
        self.dropout = dropout

    def forward(self, x, edge_index, batch=None):
        x = F.relu(self.conv1(x, edge_index))
        x = F.dropout(x, p=self.dropout, training=self.training)
        x = self.conv2(x, edge_index)
        return x

# === FingerprintNetMLP (Strong Version) ===
class FingerprintNetMLP(nn.Module):
    def __init__(self, in_dim=64, hidden_dim=128, out_dim=64):
        super(FingerprintNetMLP, self).__init__()
        self.mlp = nn.Sequential(
            nn.Linear(in_dim, hidden_dim),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, out_dim)
        )

    def forward(self, x):
        return self.mlp(x)

# === Load Variant Graphs ===
def load_variants(variant_dir):
    graphs = []
    for fname in sorted(os.listdir(variant_dir)):
        if fname.endswith(".pt"):
            path = os.path.join(variant_dir, fname)
            g = torch.load(path, weights_only=False)  # Safe for PyTorch 2.6+
            graphs.append(g)
    return graphs

# === Extract Fingerprints from a Batch ===
def extract_fp(victim_model, U_net, graphs, batch_size=32, device='cuda' if torch.cuda.is_available() else 'cpu'):
    victim_model.eval()
    U_net.eval()
    fingerprints = []

    loader = DataLoader(graphs, batch_size=batch_size, shuffle=False)
    for batch in loader:
        batch = batch.to(device)
        x = F.relu(victim_model.conv1(batch.x, batch.edge_index))
        x = F.dropout(x, p=victim_model.dropout, training=False)
        x = victim_model.conv2(x, batch.edge_index)
        pooled = global_mean_pool(x, batch.batch)
        out = U_net(pooled)
        fingerprints.append(out.detach().cpu())

    return torch.cat(fingerprints, dim=0)

# === Initialize Models ===
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

victim = SAGEMean().to(device)
victim.load_state_dict(torch.load("/content/drive/MyDrive/gnnfingers/victims/victim_Cora_SAGEMean.pth"))
print("‚úÖ Loaded victim model")

graphs = load_variants("/content/drive/MyDrive/gnnfingers/variants/Cora_SAGEMean")
print(f"‚úÖ Loaded {len(graphs)} graphs")

# === Chunked Fingerprint Extraction ===
U_net = FingerprintNetMLP().to(device)
fingerprints = []

for i in range(0, len(graphs), 100):
    chunk = graphs[i:i+100]
    print(f"üîÑ Processing graphs {i} to {i+len(chunk)-1}...")
    chunk_fp = extract_fp(victim, U_net, chunk, batch_size=32)
    fingerprints.append(chunk_fp)

# Concatenate and save
fps = torch.cat(fingerprints, dim=0)
os.makedirs("/content/drive/MyDrive/gnnfingers/fingerprints", exist_ok=True)
torch.save(fps, "/content/drive/MyDrive/gnnfingers/fingerprints/fps_Cora_SAGEMean.pt")
print("‚úÖ All fingerprints saved to: /content/drive/MyDrive/gnnfingers/fingerprints/fps_Cora_SAGEMean.pt")

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import TensorDataset, DataLoader

# Load fingerprint matrix from Drive
fps = torch.load("/content/drive/MyDrive/gnnfingers/fingerprints/fps_Cora_SAGEMean.pt")
print("‚úÖ Loaded fingerprints:", fps.shape)  # should be [400, 64]

# Labels: 200 positive, 200 negative
labels = torch.cat([torch.ones(200), torch.zeros(200)]).long()

# Train/test split
train_fps, test_fps = fps[:320], fps[320:]
train_labels, test_labels = labels[:320], labels[320:]

# Data loaders
train_ds = TensorDataset(train_fps, train_labels)
test_ds = TensorDataset(test_fps, test_labels)
train_loader = DataLoader(train_ds, batch_size=32, shuffle=True)
test_loader = DataLoader(test_ds, batch_size=32)

# Define Univerifier (stronger)
class FingerprintNetMLP(nn.Module):
    def __init__(self, in_dim=64, hidden_dim=128, out_dim=2):
        super(FingerprintNetMLP, self).__init__()
        self.mlp = nn.Sequential(
            nn.Linear(in_dim, hidden_dim),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, out_dim)
        )
    def forward(self, x):
        return self.mlp(x)

# Train the model
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
U = FingerprintNetMLP().to(device)
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(U.parameters(), lr=1e-3)

U.train()
for epoch in range(1, 21):  # 20 epochs
    total_loss = 0
    for x_batch, y_batch in train_loader:
        x_batch, y_batch = x_batch.to(device), y_batch.to(device)
        optimizer.zero_grad()
        out = U(x_batch)
        loss = criterion(out, y_batch)
        loss.backward()
        optimizer.step()
        total_loss += loss.item()
    print(f"Epoch {epoch:02d} - Loss: {total_loss:.4f}")

# Evaluate
U.eval()
correct = 0
total = 0
with torch.no_grad():
    for x_batch, y_batch in test_loader:
        x_batch, y_batch = x_batch.to(device), y_batch.to(device)
        preds = U(x_batch).argmax(dim=1)
        correct += (preds == y_batch).sum().item()
        total += y_batch.size(0)

acc = correct / total
print(f"‚úÖ Test Accuracy: {acc:.4f}")

# Save model to Drive
torch.save(U.state_dict(), "/content/drive/MyDrive/gnnfingers/checkpoints/U_Cora_SAGEMean.pth")
print("‚úÖ Univerifier saved to: /content/drive/MyDrive/gnnfingers/checkpoints/U_Cora_SAGEMean.pth")

import os
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch_geometric.nn import SAGEConv, global_mean_pool
from torch_geometric.loader import DataLoader

# === Define victim model ===
class SAGEMean(nn.Module):
    def __init__(self, in_channels=1433, hidden_channels=128, out_channels=128, dropout=0.5):
        super().__init__()
        self.conv1 = SAGEConv(in_channels, hidden_channels, aggr='mean')
        self.conv2 = SAGEConv(hidden_channels, out_channels, aggr='mean')
        self.dropout = dropout

    def forward(self, x, edge_index, batch):
        x = F.relu(self.conv1(x, edge_index))
        x = F.dropout(x, p=self.dropout, training=self.training)
        x = self.conv2(x, edge_index)
        return x

# === Fingerprint network ===
class FingerprintNetMLP(nn.Module):
    def __init__(self, in_dim=128, hidden_dim=64, out_dim=64):
        super().__init__()
        self.mlp = nn.Sequential(
            nn.Linear(in_dim, hidden_dim),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(hidden_dim, out_dim)
        )

    def forward(self, x):
        return self.mlp(x)

# === Load variant graphs ===
from torch_geometric.data import Data
import torch.serialization  ##

def load_variants(variant_dir):
    graphs = []
    torch.serialization.add_safe_globals([Data])  # ‚Üê allow loading PyG Data objects
    for fname in sorted(os.listdir(variant_dir)):
        if fname.endswith(".pt"):
            g = torch.load(os.path.join(variant_dir, fname), weights_only=False)
            graphs.append(g)
    return graphs


# === Fingerprint extraction ===
def extract_fp(victim, U_net, graphs, device='cuda' if torch.cuda.is_available() else 'cpu'):
    victim.eval(); U_net.eval()
    loader = DataLoader(graphs, batch_size=32, shuffle=False)
    out_list = []

    for batch in loader:
        batch = batch.to(device)
        x = F.relu(victim.conv1(batch.x, batch.edge_index))
        x = F.dropout(x, p=victim.dropout, training=False)
        x = victim.conv2(x, batch.edge_index)
        pooled = global_mean_pool(x, batch.batch)
        out = U_net(pooled)
        out_list.append(out.detach().cpu())

    return torch.cat(out_list, dim=0)

# === Main multi-model extraction ===
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
variant_dir = "/content/drive/MyDrive/gnnfingers/variants/Cora_SAGEMean"
graphs = load_variants(variant_dir)
print(f"‚úÖ Loaded {len(graphs)} graphs")

all_fps = []

for seed in range(5):
    print(f"üîÅ Training model #{seed+1}")
    torch.manual_seed(seed)

    model = SAGEMean().to(device)
    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)

    # Load the real Cora data to train on
    from torch_geometric.datasets import Planetoid
    dataset = Planetoid(root='/tmp/Cora', name='Cora')
    data = dataset[0].to(device)

    for epoch in range(50):  # keep it short for faster execution
        model.train()
        optimizer.zero_grad()
        out = model(data.x, data.edge_index, torch.zeros_like(data.x[:, 0], dtype=torch.long))
        loss = F.cross_entropy(out, data.y)
        loss.backward()
        optimizer.step()

    print(f"‚úÖ Trained model #{seed+1}")

    # Fingerprint extractor
    U_net = FingerprintNetMLP().to(device)
    fps = extract_fp(model, U_net, graphs, device)
    all_fps.append(fps)

# === Final concatenated fingerprint matrix ===
fingerprint_matrix = torch.cat(all_fps, dim=1)  # shape: [400, 320]
print("‚úÖ Final fingerprint shape:", fingerprint_matrix.shape)

# Save to Drive
save_path = "/content/drive/MyDrive/gnnfingers/fingerprints/fps_Cora_SAGEMean_multi.pt"
torch.save(fingerprint_matrix, save_path)
print(f"‚úÖ Saved to: {save_path}")

import os
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch_geometric.loader import DataLoader
from torch_geometric.nn import SAGEConv, global_mean_pool
from torch_geometric.data import Data
import torch.serialization

# Allowlist the PyG Data class
torch.serialization.add_safe_globals([Data])

# Load small batch of variants
def load_partial_variants(variant_dir, start, end):
    fnames = sorted([f for f in os.listdir(variant_dir) if f.endswith(".pt")])[start:end]
    return [torch.load(os.path.join(variant_dir, f), weights_only=False) for f in fnames]

# SAGEMean Model
class SAGEMean(nn.Module):
    def __init__(self, in_channels=1433, hidden_channels=128, out_channels=128, dropout=0.5):
        super().__init__()
        self.conv1 = SAGEConv(in_channels, hidden_channels)
        self.conv2 = SAGEConv(hidden_channels, out_channels)
        self.dropout = dropout

    def forward(self, x, edge_index):
        x = F.relu(self.conv1(x, edge_index))
        x = F.dropout(x, p=self.dropout, training=self.training)
        return self.conv2(x, edge_index)

# Fingerprint Network
class FingerprintNetMLP(nn.Module):
    def __init__(self, in_dim=128, hidden_dim=64, out_dim=64):
        super().__init__()
        self.mlp = nn.Sequential(
            nn.Linear(in_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, out_dim)
        )

    def forward(self, x):
        return self.mlp(x)

# Fingerprint extraction
def extract_fp(model, U_net, graphs, batch_size=32, device="cuda"):
    model.eval()
    U_net.eval()
    fps = []
    loader = DataLoader(graphs, batch_size=batch_size, shuffle=False)
    for batch in loader:
        batch = batch.to(device)
        x = F.relu(model.conv1(batch.x, batch.edge_index))
        x = F.dropout(x, p=model.dropout, training=False)
        x = model.conv2(x, batch.edge_index)
        pooled = global_mean_pool(x, batch.batch)
        out = U_net(pooled)
        fps.append(out.cpu().detach())
    return torch.cat(fps, dim=0)

# Main loop (in batches)
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
variant_dir = "/content/drive/MyDrive/gnnfingers/variants/Cora_SAGEMean"
output_dir = "/content/drive/MyDrive/gnnfingers/fingerprints"
os.makedirs(output_dir, exist_ok=True)

# Make sure to use correct dimensions
victim = SAGEMean(in_channels=1433, hidden_channels=64, out_channels=64).to(device)
victim.load_state_dict(torch.load("/content/drive/MyDrive/gnnfingers/victims/victim_Cora_SAGEMean.pth"))
U = FingerprintNetMLP(in_dim=64).to(device)


total = 400
batch_size = 100
all_fps = []

for i in range(0, total, batch_size):
    print(f"üîÑ Processing graphs {i} to {i+batch_size-1}")
    graphs = load_partial_variants(variant_dir, i, i+batch_size)
    fps = extract_fp(victim, U, graphs, device=device)
    torch.save(fps, f"{output_dir}/fps_Cora_SAGEMean_batch{i}.pt")
    all_fps.append(fps)

# Optionally merge and save
merged_fps = torch.cat(all_fps, dim=0)
torch.save(merged_fps, f"{output_dir}/fps_Cora_SAGEMean.pt")
print(f"‚úÖ Saved all fingerprints to {output_dir}/fps_Cora_SAGEMean.pt")

fps = torch.load("/content/drive/MyDrive/gnnfingers/fingerprints/fps_Cora_SAGEMean.pt")
print(fps.shape)

labels = torch.cat([torch.ones(200), torch.zeros(200)])  # 1 for positive, 0 for negative

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import DataLoader, TensorDataset

# Define the MLP Univerifier
class FingerprintNetMLP(nn.Module):
    def __init__(self, in_dim=64, hidden_dim=32, out_dim=2):
        super(FingerprintNetMLP, self).__init__()
        self.model = nn.Sequential(
            nn.Linear(in_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, out_dim)
        )

    def forward(self, x):
        return self.model(x)

# Load fingerprint data
fps = torch.load("/content/drive/MyDrive/gnnfingers/fingerprints/fps_Cora_SAGEMean.pt")
labels = torch.cat([torch.ones(200), torch.zeros(200)]).long()

# Split into train/test (80/20)
train_fps, test_fps = fps[:320], fps[320:]
train_labels, test_labels = labels[:320], labels[320:]

# Create DataLoaders
train_loader = DataLoader(TensorDataset(train_fps, train_labels), batch_size=32, shuffle=True)
test_loader = DataLoader(TensorDataset(test_fps, test_labels), batch_size=32)

# Training setup
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = FingerprintNetMLP().to(device)
opt = torch.optim.Adam(model.parameters(), lr=0.001)
criterion = nn.CrossEntropyLoss()

# Training loop
print("üîÅ Training Univerifier...")
for epoch in range(1, 21):
    model.train()
    total_loss = 0
    for x_batch, y_batch in train_loader:
        x_batch, y_batch = x_batch.to(device), y_batch.to(device)
        out = model(x_batch)
        loss = criterion(out, y_batch)
        opt.zero_grad()
        loss.backward()
        opt.step()
        total_loss += loss.item()

    print(f"Epoch {epoch:02d} - Loss: {total_loss:.4f}")

# Evaluation
model.eval()
correct = 0
total = 0
with torch.no_grad():
    for x_batch, y_batch in test_loader:
        x_batch, y_batch = x_batch.to(device), y_batch.to(device)
        out = model(x_batch)
        pred = out.argmax(dim=1)
        correct += (pred == y_batch).sum().item()
        total += y_batch.size(0)

accuracy = correct / total
print(f"‚úÖ Test Accuracy: {accuracy:.4f}")

# Save
torch.save(model.state_dict(), "/content/drive/MyDrive/gnnfingers/checkpoints/U_Cora_SAGEMean.pth")
print("‚úÖ Univerifier saved to Drive at checkpoints/U_Cora_SAGEMean.pth")

print(torch.allclose(fps[0], fps[1]))   # Should be False
print(fps[:5])  # Print first few rows to inspect variation

unique_fps = torch.unique(fps, dim=0)
print(f"Unique fingerprints: {unique_fps.size(0)} out of {fps.size(0)}")

import torch
import torch.nn.functional as F
from torch_geometric.nn import SAGEConv

class SAGEMean(torch.nn.Module):
    def __init__(self, input_dim=1433, hidden_dim=64, output_dim=7, dropout=0.5):
        super(SAGEMean, self).__init__()
        self.conv1 = SAGEConv(input_dim, hidden_dim, aggr='mean')
        self.conv2 = SAGEConv(hidden_dim, hidden_dim, aggr='mean')
        self.classifier = torch.nn.Linear(hidden_dim, output_dim)
        self.dropout = dropout

    def forward(self, x, edge_index):
        x = self.conv1(x, edge_index)
        x = F.relu(x)
        x = F.dropout(x, p=self.dropout, training=self.training)
        x = self.conv2(x, edge_index)
        x = F.relu(x)
        return x

import torch
import torch.nn.functional as F
from torch_geometric.datasets import Planetoid
from torch_geometric.loader import DataLoader
model = SAGEMean().to(device)

import os
import random

# Load dataset
dataset = Planetoid(root='/tmp/Cora', name='Cora')
data = dataset[0].to('cuda' if torch.cuda.is_available() else 'cpu')

# Create save directory
variant_dir = "variants/Cora_SAGEMean"
os.makedirs(variant_dir, exist_ok=True)

# Training function
def train(model, data, epochs=50):
    model.train()
    optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)
    for epoch in range(epochs):
        optimizer.zero_grad()
        out = model(data.x, data.edge_index)
        loss = F.nll_loss(out[data.train_mask], data.y[data.train_mask])
        loss.backward()
        optimizer.step()

# Train and save 200 models
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

for i in range(200):
    torch.manual_seed(i)  # Ensure varied initialization
    model = SAGEMean().to(device)
    train(model, data)
    torch.save(model.state_dict(), os.path.join(variant_dir, f"pos_{i}.pth"))
    print(f"‚úÖ Saved variant {i+1}/200")

print("üéâ Done generating 200 positive SAGEMean variants.")

import torch
import torch.nn.functional as F
from torch_geometric.loader import DataLoader
from torch_geometric.nn import global_mean_pool

def extract_fp(model, U_net, graphs, device='cpu'):
    model.eval()
    U_net.eval()
    all_fps = []

    loader = DataLoader(graphs, batch_size=32, shuffle=False)
    for batch in loader:
        batch = batch.to(device)
        x = F.relu(model.conv1(batch.x, batch.edge_index))
        x = F.dropout(x, p=0.5, training=False)
        x = model.conv2(x, batch.edge_index)
        x = global_mean_pool(x, batch.batch)
        fp = U_net(x)
        all_fps.append(fp.detach().cpu())

    return torch.cat(all_fps, dim=0)

import os

# Should return True if file exists
print(os.path.isfile("/content/drive/MyDrive/gnnfingers/utils.py"))

utils_code = """
import torch
import torch.nn.functional as F
from torch_geometric.loader import DataLoader

# Extract fingerprint from a model for a list of graphs
def extract_fp(victim_model, U_net, graphs, device='cpu'):
    victim_model.eval()
    U_net.eval()
    all_fp = []

    loader = DataLoader(graphs, batch_size=32, shuffle=False)
    for batch in loader:
        batch = batch.to(device)
        x = F.relu(victim_model.conv1(batch.x, batch.edge_index))
        x = F.dropout(x, p=victim_model.dropout, training=False)
        x = victim_model.conv2(x, batch.edge_index)
        x = torch_geometric.nn.global_mean_pool(x, batch.batch)
        fp = U_net(x)
        all_fp.append(fp.detach().cpu())
    return torch.cat(all_fp, dim=0)

# Load multiple .pt graph files as a list
def load_variants(variant_dir):
    import os
    graphs = []
    for fname in sorted(os.listdir(variant_dir)):
        if fname.endswith(".pt"):
            g = torch.load(os.path.join(variant_dir, fname), map_location='cpu')
            graphs.append(g)
    return graphs
"""

# Save to Drive
with open("/content/drive/MyDrive/gnnfingers/utils.py", "w") as f:
    f.write(utils_code)

print("‚úÖ utils.py created in gnngfingers folder.")

import sys
sys.path.append("/content/drive/MyDrive/gnnfingers")
from utils import extract_fp, load_variants

model_code = """
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch_geometric.nn import SAGEConv, global_mean_pool

class SAGEMean(nn.Module):
    def __init__(self, in_channels=1433, hidden_channels=128, out_channels=7, dropout=0.5):
        super(SAGEMean, self).__init__()
        self.conv1 = SAGEConv(in_channels, hidden_channels)
        self.conv2 = SAGEConv(hidden_channels, hidden_channels)
        self.dropout = dropout
        self.classifier = nn.Linear(hidden_channels, out_channels)

    def forward(self, x, edge_index, batch=None):
        x = self.conv1(x, edge_index)
        x = F.relu(x)
        x = F.dropout(x, p=self.dropout, training=self.training)
        x = self.conv2(x, edge_index)
        if batch is not None:
            x = global_mean_pool(x, batch)
        return self.classifier(x)

class FingerprintNetMLP(nn.Module):
    def __init__(self, input_dim=128, hidden_dim=64):
        super(FingerprintNetMLP, self).__init__()
        self.mlp = nn.Sequential(
            nn.Linear(input_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, 2)
        )

    def forward(self, x):
        return self.mlp(x)
"""
with open("/content/drive/MyDrive/gnnfingers/models.py", "w") as f:
    f.write(model_code)

print("‚úÖ models.py created.")

import importlib
import models
importlib.reload(models)

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch_geometric.nn import SAGEConv

class SAGEMean(nn.Module):
    def __init__(self, in_channels=1433, hidden_channels=64, out_channels=7, dropout=0.5):
        super(SAGEMean, self).__init__()
        self.conv1 = SAGEConv(in_channels, hidden_channels)
        self.conv2 = SAGEConv(hidden_channels, hidden_channels)
        self.classifier = nn.Linear(hidden_channels, out_channels)
        self.dropout = dropout

    def forward(self, x, edge_index):
        x = F.relu(self.conv1(x, edge_index))
        x = F.dropout(x, p=self.dropout, training=self.training)
        x = F.relu(self.conv2(x, edge_index))
        x = F.dropout(x, p=self.dropout, training=self.training)
        x = self.classifier(x)
        return x

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch_geometric.nn import SAGEConv

class SAGEMean(nn.Module):
    def __init__(self, in_channels=1433, hidden_channels=64, dropout=0.5):
        super(SAGEMean, self).__init__()
        self.conv1 = SAGEConv(in_channels, hidden_channels)
        self.conv2 = SAGEConv(hidden_channels, hidden_channels)
        self.dropout = dropout

    def forward(self, x, edge_index):
        x = F.relu(self.conv1(x, edge_index))
        x = F.dropout(x, p=self.dropout, training=self.training)
        x = F.relu(self.conv2(x, edge_index))
        x = F.dropout(x, p=self.dropout, training=self.training)
        return x  # No classifier

victim = SAGEMean().to(device)
victim.load_state_dict(
    torch.load("/content/drive/MyDrive/gnnfingers/victims/victim_Cora_SAGEMean.pth"),
    strict=False  # This avoids crashing due to missing classifier
)

class SAGEMean(torch.nn.Module):
    def __init__(self, in_channels=1433, hidden_channels=64, out_channels=7, dropout=0.5):
        super().__init__()
        self.conv1 = SAGEConv(in_channels, hidden_channels)
        self.conv2 = SAGEConv(hidden_channels, hidden_channels)
        self.classifier = nn.Linear(hidden_channels, out_channels)
        self.dropout = dropout

    def forward(self, x, edge_index):
        x = self.conv1(x, edge_index).relu()
        x = F.dropout(x, p=self.dropout, training=self.training)
        x = self.conv2(x, edge_index).relu()
        x = F.dropout(x, p=self.dropout, training=self.training)
        return self.classifier(x)

from torch.serialization import add_safe_globals
from torch_geometric.data import Data
from torch_geometric.data.data import DataEdgeAttr, DataTensorAttr

add_safe_globals([Data, DataEdgeAttr, DataTensorAttr])

!ls /content/drive/MyDrive/gnnfingers/

from torch.serialization import add_safe_globals
from torch_geometric.data import Data
from torch_geometric.data.storage import GlobalStorage
from torch_geometric.data.data import DataEdgeAttr, DataTensorAttr

# Allowlist required PyG classes
add_safe_globals([Data, GlobalStorage, DataEdgeAttr, DataTensorAttr])

from utils import load_variants

variant_dir = "/content/drive/MyDrive/gnnfingers/variants/Cora_SAGEMean"
graphs = load_variants(variant_dir)
print(f"‚úÖ Loaded {len(graphs)} graphs")

!ls /content/drive/MyDrive/gnnfingers/victims/

import sys
sys.path.append('/content/drive/MyDrive/gnnfingers')

import torch
from torch.utils.data import DataLoader

from utils import load_variants, extract_fp
from models import SAGEMean, FingerprintNetMLP

from torch_geometric.data import Data
from torch_geometric.data.storage import GlobalStorage
from torch_geometric.data.data import DataEdgeAttr, DataTensorAttr
from torch.serialization import add_safe_globals

# Allow custom PyG types for unpickling
add_safe_globals([Data, GlobalStorage, DataEdgeAttr, DataTensorAttr])

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

victim = SAGEMean(in_channels=1433, hidden_channels=64, out_channels=7).to(device)
victim.load_state_dict(
    torch.load("/content/drive/MyDrive/gnnfingers/victims/victim_Cora_SAGEMean.pth", map_location=device),
    strict=False
)
victim.eval()

variant_dir = "/content/drive/MyDrive/gnnfingers/variants/Cora_SAGEMean"
graphs = load_variants(variant_dir)
print(f"‚úÖ Loaded {len(graphs)} graphs")

# Step 1: Mount drive if not done already
from google.colab import drive
drive.mount('/content/drive')

# Step 2: Add project folder to Python path
import sys
sys.path.append('/content/drive/MyDrive/gnnfingers')

# Step 3: Re-import everything from your modules
from utils import load_variants, extract_fp
from models import SAGEMean, FingerprintNetMLP

import torch
from torch.utils.data import DataLoader

# Step 4: Add PyG-safe globals for unpickling
from torch_geometric.data import Data
from torch_geometric.data.storage import GlobalStorage
from torch_geometric.data.data import DataEdgeAttr, DataTensorAttr
from torch.serialization import add_safe_globals

add_safe_globals([Data, GlobalStorage, DataEdgeAttr, DataTensorAttr])

class SAGEMean(nn.Module):
    def __init__(self, in_channels, hidden_channels):
        super().__init__()
        self.conv1 = SAGEConv(in_channels, hidden_channels)
        self.conv2 = SAGEConv(hidden_channels, hidden_channels)
        self.lin = nn.Linear(hidden_channels, 7)

victim = SAGEMean(1433, 64).to(device)
victim.load_state_dict(torch.load("/content/drive/MyDrive/gnnfingers/victims/victim_Cora_SAGEMean.pth", map_location=device), strict=False)
victim.eval()

import torch
import torch.nn.functional as F
from torch_geometric.nn import SAGEConv
from torch import nn

class SAGEMean(nn.Module):
    def __init__(self, in_channels, hidden_channels, dropout=0.5):
        super().__init__()
        self.conv1 = SAGEConv(in_channels, hidden_channels)
        self.conv2 = SAGEConv(hidden_channels, hidden_channels)
        self.lin = nn.Linear(hidden_channels, 7)  # assuming 7 classes for Cora
        self.dropout = dropout

    def forward(self, x, edge_index):
        x = self.conv1(x, edge_index)
        x = F.relu(x)
        x = F.dropout(x, p=self.dropout, training=self.training)
        x = self.conv2(x, edge_index)
        x = F.relu(x)
        x = F.dropout(x, p=self.dropout, training=self.training)
        return self.lin(x)

import torch.nn as nn

class FingerprintNetMLP(nn.Module):
    def __init__(self, in_dim=64, hidden_dims=[128, 64], out_dim=2):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(in_dim, hidden_dims[0]),
            nn.LeakyReLU(),
            nn.Linear(hidden_dims[0], hidden_dims[1]),
            nn.LeakyReLU(),
            nn.Linear(hidden_dims[1], out_dim),
            nn.Softmax(dim=1)
        )

    def forward(self, x):
        return self.net(x)

import os
import torch

def load_variants(variant_dir):
    graphs = []
    for fname in sorted(os.listdir(variant_dir)):
        if fname.endswith(".pt"):
            graph = torch.load(os.path.join(variant_dir, fname))
            graphs.append(graph)
    return graphs

from torch_geometric.data.storage import GlobalStorage
add_safe_globals([GlobalStorage])  # <-- Add this
from torch_geometric.data import Data
from torch_geometric.data.data import DataEdgeAttr, DataTensorAttr
from torch_geometric.data.storage import GlobalStorage
from torch.serialization import add_safe_globals

add_safe_globals([Data, DataEdgeAttr, DataTensorAttr, GlobalStorage])

all_graphs = load_variants("/content/drive/MyDrive/gnnfingers/variants/Cora_SAGEMean")

import torch
import os
from tqdm import tqdm
from torch_geometric.data import Data
from torch_geometric.data.data import DataEdgeAttr, DataTensorAttr
from torch_geometric.data.storage import GlobalStorage
from torch.serialization import add_safe_globals
from torch_geometric.nn import global_mean_pool
from torch_geometric.loader import DataLoader

# Allow PyG globals for unpickling
add_safe_globals([Data, DataEdgeAttr, DataTensorAttr, GlobalStorage])

# Redefine SAGEMean
class SAGEMean(torch.nn.Module):
    def __init__(self, in_channels, hidden_channels, dropout=0.5):
        super().__init__()
        from torch_geometric.nn import SAGEConv
        self.conv1 = SAGEConv(in_channels, hidden_channels)
        self.conv2 = SAGEConv(hidden_channels, hidden_channels)
        self.lin = torch.nn.Linear(hidden_channels, 7)
        self.dropout = dropout

    def forward(self, x, edge_index, batch=None):
        x = self.conv1(x, edge_index)
        x = torch.relu(x)
        x = torch.nn.functional.dropout(x, p=self.dropout, training=self.training)
        x = self.conv2(x, edge_index)
        x = torch.relu(x)
        if batch is not None:
            x = global_mean_pool(x, batch)
        return self.lin(x)

# FingerprintNetMLP
class FingerprintNetMLP(torch.nn.Module):
    def __init__(self, in_dim=64, hidden_dims=[128, 64], out_dim=2):
        super().__init__()
        self.net = torch.nn.Sequential(
            torch.nn.Linear(in_dim, hidden_dims[0]),
            torch.nn.LeakyReLU(),
            torch.nn.Linear(hidden_dims[0], hidden_dims[1]),
            torch.nn.LeakyReLU(),
            torch.nn.Linear(hidden_dims[1], out_dim),
            torch.nn.Softmax(dim=1)
        )
    def forward(self, x):
        return self.net(x)

# Load victim + fingerprint model
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
victim = SAGEMean(1433, 64).to(device)
victim.load_state_dict(torch.load("/content/drive/MyDrive/gnnfingers/victims/victim_Cora_SAGEMean.pth", map_location=device), strict=False)
victim.eval()
fp_net = FingerprintNetMLP().to(device)
fp_net.eval()

# Variant directory
variant_dir = "/content/drive/MyDrive/gnnfingers/variants/Cora_SAGEMean"
output_dir = "/content/drive/MyDrive/gnnfingers/fingerprints/safe_chunks"
os.makedirs(output_dir, exist_ok=True)

# Process one graph at a time, saving fingerprint every 10
batch_size = 10
fingerprints = []
filenames = sorted([f for f in os.listdir(variant_dir) if f.endswith(".pt")])

with torch.no_grad():
    for i, fname in enumerate(tqdm(filenames)):
        out_file = os.path.join(output_dir, f"chunk_{i // batch_size}.pt")
        if os.path.exists(out_file):
            continue  # skip already saved

        # Load one graph
        graph = torch.load(os.path.join(variant_dir, fname), map_location=device)
        loader = DataLoader([graph], batch_size=1)
        for batch in loader:
            batch = batch.to(device)
            x = torch.relu(victim.conv1(batch.x, batch.edge_index))
            x = torch.nn.functional.dropout(x, p=victim.dropout, training=False)
            x = victim.conv2(x, batch.edge_index)
            x = global_mean_pool(x, batch.batch)
            fp = fp_net(x)
            fingerprints.append(fp.cpu())

        # Save after every 10 graphs
        if (i + 1) % batch_size == 0 or (i + 1) == len(filenames):
            chunk_tensor = torch.cat(fingerprints, dim=0)
            torch.save(chunk_tensor, out_file)
            print(f"‚úÖ Saved {out_file}")
            fingerprints = []  # clear memory

import glob
all_chunks = sorted(glob.glob("/content/drive/MyDrive/gnnfingers/fingerprints/safe_chunks/chunk_*.pt"))
final = torch.cat([torch.load(f) for f in all_chunks], dim=0)
torch.save(final, "/content/drive/MyDrive/gnnfingers/fingerprints/Cora_SAGEMean.pt")
print("‚úÖ Final fingerprint tensor saved.")

import glob
import torch

# Get all chunk files
chunk_files = sorted(glob.glob("/content/drive/MyDrive/gnnfingers/fingerprints/safe_chunks/chunk_*.pt"))

# Load and concatenate
all_fps = [torch.load(f) for f in chunk_files]
final_tensor = torch.cat(all_fps, dim=0)

# Save final tensor
torch.save(final_tensor, "/content/drive/MyDrive/gnnfingers/fingerprints/Cora_SAGEMean.pt")
print("‚úÖ Final fingerprint tensor saved with shape:", final_tensor.shape)

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import TensorDataset, DataLoader, random_split
from sklearn.metrics import roc_auc_score

# === Load Data ===
fingerprints = torch.load("/content/drive/MyDrive/gnnfingers/fingerprints/Cora_SAGEMean.pt")
labels = torch.cat([torch.ones(200), torch.zeros(200)]).long()

# === Create Dataset & Dataloaders ===
dataset = TensorDataset(fingerprints, labels)
train_size = int(0.7 * len(dataset))
test_size = len(dataset) - train_size
train_ds, test_ds = random_split(dataset, [train_size, test_size], generator=torch.Generator().manual_seed(42))
train_loader = DataLoader(train_ds, batch_size=32, shuffle=True)
test_loader = DataLoader(test_ds, batch_size=64)

# === Define Univerifier ===
class Univerifier(nn.Module):
    def __init__(self, in_dim=2, hidden_dim=16):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(in_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, 2)
        )

    def forward(self, x):
        return self.net(x)

# === Training Setup ===
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = Univerifier().to(device)
optimizer = torch.optim.Adam(model.parameters(), lr=0.01)
criterion = nn.CrossEntropyLoss()

# === Training Loop ===
for epoch in range(1, 31):
    model.train()
    total_loss = 0
    for xb, yb in train_loader:
        xb, yb = xb.to(device), yb.to(device)
        optimizer.zero_grad()
        preds = model(xb)
        loss = criterion(preds, yb)
        loss.backward()
        optimizer.step()
        total_loss += loss.item()

    # === Evaluation ===
    model.eval()
    all_probs = []
    all_targets = []
    with torch.no_grad():
        for xb, yb in test_loader:
            xb = xb.to(device)
            logits = model(xb)
            probs = F.softmax(logits, dim=1)[:, 1].cpu()
            all_probs.append(probs)
            all_targets.append(yb)

    # Compute ARUC
    probs = torch.cat(all_probs).numpy()
    targets = torch.cat(all_targets).numpy()
    aruc = roc_auc_score(targets, probs)

    print(f"Epoch {epoch:02d} | Loss: {total_loss:.4f} | ARUC: {aruc:.4f}")

torch.save(model.state_dict(), "/content/drive/MyDrive/gnnfingers/checkpoints/U_Cora_SAGEMean.pth")
print("‚úÖ Univerifier model saved.")

import matplotlib.pyplot as plt
from sklearn.metrics import roc_curve, auc

# Ensure you have predictions and true labels
fpr, tpr, thresholds = roc_curve(targets, probs)
roc_auc = auc(fpr, tpr)

# Plot
plt.figure(figsize=(6, 5))
plt.plot(fpr, tpr, color='blue', lw=2, label=f'ROC curve (ARUC = {roc_auc:.4f})')
plt.plot([0, 1], [0, 1], color='gray', lw=1, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate', fontsize=12)
plt.ylabel('True Positive Rate', fontsize=12)
plt.title('Univerifier ROC Curve - Cora SAGEMean', fontsize=13)
plt.legend(loc="lower right")
plt.grid(True)
plt.show()

print("Positive mean:", fingerprints[:200].mean(dim=0))
print("Negative mean:", fingerprints[200:].mean(dim=0))

print("Positive sample fp:", fingerprints[0])
print("Negative sample fp:", fingerprints[250])

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch_geometric.nn import SAGEConv

# Define SAGEMean
class SAGEMean(nn.Module):
    def __init__(self, in_channels, hidden_channels, out_channels):
        super(SAGEMean, self).__init__()
        self.conv1 = SAGEConv(in_channels, hidden_channels)
        self.conv2 = SAGEConv(hidden_channels, out_channels)

    def forward(self, x, edge_index, batch):
        x = self.conv1(x, edge_index)
        x = F.relu(x)
        x = self.conv2(x, edge_index)
        return x

# Define FingerprintNetMLP
class FingerprintNetMLP(nn.Module):
    def __init__(self, in_dim=7, hidden_dim=32, out_dim=2):
        super(FingerprintNetMLP, self).__init__()
        self.fc1 = nn.Linear(in_dim, hidden_dim)
        self.fc2 = nn.Linear(hidden_dim, out_dim)

    def forward(self, x):
        x = F.relu(self.fc1(x))
        return self.fc2(x)

def load_variants(folder, keyword):
    files = sorted([f for f in os.listdir(folder) if keyword in f])
    graphs = [torch.load(os.path.join(folder, f)) for f in files]
    return graphs

pos_graphs = load_variants("/content/drive/MyDrive/gnnfingers/variants/Cora_SAGEMean", "pos")
neg_graphs = load_variants("/content/drive/MyDrive/gnnfingers/variants/Cora_SAGEMean", "neg")

# Clean, simple version ‚Äî NO extra 'keyword' argument
def load_variants(folder):
    files = sorted(os.listdir(folder))
    graphs = []
    for f in files:
        path = os.path.join(folder, f)
        graphs.append(torch.load(path))
    return graphs

from google.colab import drive
drive.mount('/content/drive')

!ls /content/drive/MyDrive/gnnfingers/variants/Cora_SAGEMean/

import os
import torch

# Load function that filters by prefix
def load_variants_by_prefix(folder, prefix, max_files=None):
    files = sorted([f for f in os.listdir(folder) if f.startswith(prefix)])
    if max_files:
        files = files[:max_files]
    graphs = []
    for f in files:
        path = os.path.join(folder, f)
        graphs.append(torch.load(path))
    return graphs

# Set paths
variant_dir = "/content/drive/MyDrive/gnnfingers/variants/Cora_SAGEMean"

# Load a subset of 50 each (adjust number as needed: 10, 100, 200)
num_variants = 50
pos_graphs = load_variants_by_prefix(variant_dir, "P_", num_variants)
neg_graphs = load_variants_by_prefix(variant_dir, "N_", num_variants)

# Label tensors
labels = torch.cat([torch.ones(len(pos_graphs)), torch.zeros(len(neg_graphs))]).long()

print(f"‚úÖ Loaded {len(pos_graphs)} positive and {len(neg_graphs)} negative graphs.")

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch_geometric.nn import global_mean_pool
from sklearn.metrics import accuracy_score

# Assuming `victim` is already loaded and on the correct device
# And that `pos_graphs`, `neg_graphs`, and `labels` are ready

graphs = pos_graphs + neg_graphs
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# ---- Extract embeddings from the victim ----
def extract_embeddings(model, graphs):
    model.eval()
    embs = []
    with torch.no_grad():
        for g in graphs:
            g = g.to(device)
            x = model(g.x, g.edge_index)
            pooled = global_mean_pool(x, g.batch)
            embs.append(pooled.cpu())
    return torch.cat(embs)

X = extract_embeddings(victim, graphs)
y = labels

# ---- Train FingerprintNet ----
class FingerprintNetMLP(nn.Module):
    def __init__(self, in_dim=64, hidden_dim=32, out_dim=2):
        super(FingerprintNetMLP, self).__init__()
        self.fc1 = nn.Linear(in_dim, hidden_dim)
        self.fc2 = nn.Linear(hidden_dim, out_dim)

    def forward(self, x):
        x = F.relu(self.fc1(x))
        return self.fc2(x)

fp_net = FingerprintNetMLP().to(device)
optimizer = torch.optim.Adam(fp_net.parameters(), lr=0.001)
loss_fn = nn.CrossEntropyLoss()

# Train for 30 epochs
for epoch in range(1, 31):
    fp_net.train()
    logits = fp_net(X.to(device))
    loss = loss_fn(logits, y.to(device))

    optimizer.zero_grad()
    loss.backward()
    optimizer.step()

    if epoch % 5 == 0 or epoch == 1:
        fp_net.eval()
        pred = torch.argmax(fp_net(X.to(device)), dim=1)
        acc = accuracy_score(y.cpu(), pred.cpu())
        print(f"Epoch {epoch:02d} - Loss: {loss.item():.4f} - Accuracy: {acc:.4f}")

# Save trained model
torch.save(fp_net.state_dict(), "/content/drive/MyDrive/gnnfingers/checkpoints/fpnet_Cora_SAGEMean_small.pth")
print("‚úÖ FingerprintNet model saved.")

import matplotlib.pyplot as plt
from sklearn.decomposition import PCA

# Perform PCA to 2D for visualization
pca = PCA(n_components=2)
X_2d = pca.fit_transform(X)

# Plot
plt.figure(figsize=(6,5))
plt.scatter(X_2d[:len(pos_graphs), 0], X_2d[:len(pos_graphs), 1], c='green', label='Positive', alpha=0.6)
plt.scatter(X_2d[len(pos_graphs):, 0], X_2d[len(pos_graphs):, 1], c='red', label='Negative', alpha=0.6)
plt.title("Fingerprint Embeddings (PCA)")
plt.legend()
plt.grid(True)
plt.show()

def extract_embeddings(model, graphs):
    embs = []
    model.eval()
    with torch.no_grad():
        for g in graphs:
            g = g.to(device)
            # Check batch attribute
            if not hasattr(g, 'batch'):
                g.batch = torch.zeros(g.num_nodes, dtype=torch.long).to(device)
            x = model(g.x, g.edge_index)
            pooled = global_mean_pool(x, g.batch)
            embs.append(pooled.cpu())
    return torch.cat(embs)

X = extract_embeddings(victim, graphs)

print("Sample fingerprint 1:", X[0])
print("Sample fingerprint 2:", X[1])
print("Mean (positive):", X[:len(pos_graphs)].mean(dim=0))
print("Mean (negative):", X[len(pos_graphs):].mean(dim=0))

from torch_geometric.loader import DataLoader

def extract_embeddings(model, graphs):
    embs = []
    model.eval()
    loader = DataLoader(graphs, batch_size=32, shuffle=False)

    with torch.no_grad():
        for batch in loader:
            batch = batch.to(device)
            x = model(batch.x, batch.edge_index)
            pooled = global_mean_pool(x, batch.batch)
            embs.append(pooled.cpu())

    return torch.cat(embs)

X = extract_embeddings(victim, graphs)

print(X[0])
print(X[1])
print(X[:len(pos_graphs)].mean(dim=0))
print(X[len(pos_graphs):].mean(dim=0))

print("Node features for graph 0:")
print(graphs[0].x)

print("\nEdge index for graph 0:")
print(graphs[0].edge_index)

print("\nBatch ID for each node in graph 0:")
print(graphs[0].batch if hasattr(graphs[0], 'batch') else "No batch attribute")

from torch_geometric.datasets import Planetoid
cora = Planetoid(root='data/Cora', name='Cora')
original_x = cora[0].x  # shape: [N, 1433]

import os
import torch

variant_dir = "/content/drive/MyDrive/gnnfingers/variants/Cora_SAGEMean"
files = sorted(os.listdir(variant_dir))

for fname in files:
    path = os.path.join(variant_dir, fname)
    g = torch.load(path)

    # Safely assign original features (if node count matches)
    if g.num_nodes == original_x.shape[0]:
        g.x = original_x.clone()
    else:
        g.x = original_x[:g.num_nodes].clone()  # truncate if smaller

    torch.save(g, path)  # Overwrite with fixed features

# Re-run victim model to extract updated embeddings
def extract_embeddings(model, graphs):
    embs = []
    model.eval()
    with torch.no_grad():
        for g in graphs:
            g = g.to(device)
            x = model(g.x, g.edge_index)
            pooled = global_mean_pool(x, g.batch)
            embs.append(pooled.cpu())
    return torch.cat(embs)

# Combine positive and negative graphs
graphs = pos_graphs + neg_graphs
labels = torch.cat([torch.ones(len(pos_graphs)), torch.zeros(len(neg_graphs))]).long()

# Extract embeddings
X = extract_embeddings(victim, graphs)
y = labels

print("‚úÖ Updated embeddings extracted.")

# Define FingerprintNet again if needed
class FingerprintNetMLP(nn.Module):
    def __init__(self, in_dim=64, hidden_dim=32, out_dim=2):
        super(FingerprintNetMLP, self).__init__()
        self.fc1 = nn.Linear(in_dim, hidden_dim)
        self.fc2 = nn.Linear(hidden_dim, out_dim)

    def forward(self, x):
        x = F.relu(self.fc1(x))
        return self.fc2(x)

# Initialize network and optimizer
fp_net = FingerprintNetMLP().to(device)
optimizer = torch.optim.Adam(fp_net.parameters(), lr=0.001)
loss_fn = nn.CrossEntropyLoss()

# Train for 50 epochs
for epoch in range(1, 51):
    fp_net.train()
    logits = fp_net(X.to(device))
    loss = loss_fn(logits, y.to(device))

    optimizer.zero_grad()
    loss.backward()
    optimizer.step()

    if epoch % 5 == 0:
        fp_net.eval()
        pred = torch.argmax(fp_net(X.to(device)), dim=1)
        acc = accuracy_score(y.cpu(), pred.cpu())
        print(f"Epoch {epoch:02d} - Loss: {loss.item():.4f} - Accuracy: {acc:.4f}")

# Save the trained model
torch.save(fp_net.state_dict(), "/content/drive/MyDrive/gnnfingers/checkpoints/fpnet_Cora_SAGEMean.pth")
print("‚úÖ Re-trained FingerprintNet saved.")

# Check if all extracted fingerprints are the same
print("Unique fingerprint rows:", torch.unique(X, dim=0).size(0))

from torch_geometric.datasets import Planetoid

dataset = Planetoid(root='/tmp/Cora', name='Cora')
original_x = dataset[0].x  # shape: [num_nodes, num_features]

for g in graphs:
    g.x = original_x.clone()

import torch.nn.functional as F
from torch_geometric.loader import DataLoader
from torch_geometric.nn import global_mean_pool

def extract_fp(model, U_net, graphs, batch_size=32):
    model.eval()
    U_net.eval()
    all_fps = []

    loader = DataLoader(graphs, batch_size=batch_size, shuffle=False)

    for batch in loader:
        batch = batch.to(next(model.parameters()).device)

        # Manual dropout added if needed (training=False always)
        x = F.relu(model.conv1(batch.x, batch.edge_index))
        x = F.dropout(x, p=0.5, training=False)  # dropout probability manually set
        x = model.conv2(x, batch.edge_index)

        pooled = global_mean_pool(x, batch.batch)
        fp = U_net(pooled)
        all_fps.append(fp.detach().cpu())

    return torch.cat(all_fps, dim=0)

# Re-extract raw fingerprints (not predictions)
fingerprints = extract_fp(victim, FingerprintNetMLP().to(device), graphs)  # returns numpy
fingerprints = torch.tensor(fingerprints, dtype=torch.float32).to(device)

# Save if needed
torch.save(fingerprints, "/content/drive/MyDrive/gnnfingers/fingerprints/Cora_SAGEMean_clean.pt")

from google.colab import drive
drive.mount('/content/drive')

import sys
sys.path.append("/content/drive/MyDrive/gnnfingers/")  # or wherever models.py is
from models import SAGEMean, FingerprintNetMLP

import torch
from models import SAGEMean  # Make sure this file is in your path

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

# This MUST match the saved model architecture
victim = SAGEMean(in_channels=1433, hidden_channels=64, out_channels=64).to(device)

# Load the correct victim model weights
victim.load_state_dict(torch.load(
    "/content/drive/MyDrive/gnnfingers/victims/victim_Cora_SAGEMean.pth",
    map_location=device
), strict=False)

victim.eval()
print("‚úÖ Victim model loaded and ready.")

class FingerprintNetMLP(nn.Module):
    def __init__(self, in_dim=2, hidden_dim=16, out_dim=2):
        super(FingerprintNetMLP, self).__init__()
        self.net = nn.Sequential(
            nn.Linear(in_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, out_dim)
        )

    def forward(self, x):
        return self.net(x)

fp_net = FingerprintNetMLP().to(device)
fp_net.load_state_dict(torch.load(
    "/content/drive/MyDrive/gnnfingers/checkpoints/U_Cora_SAGEMean.pth",
    map_location=device
))
fp_net.eval()
print("‚úÖ FingerprintNet loaded.")

fingerprints = torch.load("/content/drive/MyDrive/gnnfingers/fingerprints/Cora_SAGEMean.pt").to(device)
print("‚úÖ Fingerprints reloaded:", fingerprints.shape)

labels = torch.cat([torch.ones(50), torch.zeros(50)]).long()  # 1s for positive, 0s for negative

from torch_geometric.loader import DataLoader
import torch.nn.functional as F

def extract_fp(model, U_net, graphs, batch_size=32):
    model.eval()
    U_net.eval()
    all_fps = []

    loader = DataLoader(graphs, batch_size=batch_size, shuffle=False)
    for batch in loader:
        batch = batch.to(next(model.parameters()).device)

        # Forward pass through victim model
        x = F.relu(model.conv1(batch.x, batch.edge_index))
        x = F.dropout(x, p=0.5, training=False)
        x = model.conv2(x, batch.edge_index)

        # Pool and pass through fingerprint net
        x = torch_geometric.nn.global_mean_pool(x, batch.batch)
        out = U_net(x)
        all_fps.append(out.detach().cpu())

    return torch.cat(all_fps, dim=0)

from google.colab import drive
drive.mount('/content/drive')

import os

base_path = "/content/drive/MyDrive/gnnfingers/variants"
print("Subfolders under variants:")
print(os.listdir(base_path))

cora_path = "/content/drive/MyDrive/gnnfingers/variants/Cora_SAGEMean"
print("Subfolders under Cora_SAGEMean:")
print(os.listdir(cora_path))

from torch.serialization import add_safe_globals
from torch_geometric.data import Data
from torch_geometric.data.data import DataEdgeAttr, DataTensorAttr
from torch_geometric.data.storage import GlobalStorage

# Allowlist all necessary classes for graph deserialization
add_safe_globals([Data, DataEdgeAttr, DataTensorAttr, GlobalStorage])

variant_dir = "/content/drive/MyDrive/gnnfingers/variants/Cora_SAGEMean"
graphs = load_flat_variants(variant_dir)
print(f"‚úÖ Loaded {len(graphs)} graphs (should be 400: 200 P + 200 N)")

import torch.nn as nn
from torch_geometric.nn import SAGEConv

class SAGEMean(nn.Module):
    def __init__(self, in_channels, hidden_channels, out_channels):
        super(SAGEMean, self).__init__()
        self.conv1 = SAGEConv(in_channels, hidden_channels)
        self.conv2 = SAGEConv(hidden_channels, out_channels)

    def forward(self, x, edge_index):
        x = self.conv1(x, edge_index)
        x = torch.relu(x)
        x = self.conv2(x, edge_index)
        return x

class FingerprintNetMLP(nn.Module):
    def __init__(self):
        super(FingerprintNetMLP, self).__init__()
        self.net = nn.Sequential(
            nn.Linear(64, 32),  # 64-d fingerprint input
            nn.ReLU(),
            nn.Linear(32, 2)    # Binary classification (positive/negative)
        )

    def forward(self, x):
        return self.net(x)

import torch

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

victim = SAGEMean(in_channels=1433, hidden_channels=64, out_channels=64).to(device)
fp_net = FingerprintNetMLP().to(device)

import torch.nn as nn

class FingerprintNetMLP(nn.Module):
    def __init__(self, in_channels=2, hidden_channels=16, out_channels=2):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(in_channels, hidden_channels),
            nn.ReLU(),
            nn.Linear(hidden_channels, out_channels)
        )

    def forward(self, x):
        return self.net(x)

fp_net = FingerprintNetMLP().to(device)
fp_net.load_state_dict(torch.load(
    "/content/drive/MyDrive/gnnfingers/checkpoints/U_Cora_SAGEMean.pth",
    map_location=device
))
fp_net.eval()
print("‚úÖ FingerprintNet loaded and ready.")

from torch_geometric.loader import DataLoader
import torch.nn.functional as F

def extract_fp(model, U_net, graphs, batch_size=32):
    model.eval()
    U_net.eval()
    all_p = []

    loader = DataLoader(graphs, batch_size=batch_size, shuffle=False)

    for batch in loader:
        batch = batch.to(device)
        with torch.no_grad():
            # forward pass through victim model
            x = F.relu(model.conv1(batch.x, batch.edge_index))
            x = F.dropout(x, p=0.5, training=False)
            x = model.conv2(x, batch.edge_index)
            x = F.relu(x)

            # Global mean pooling
            pooled = torch_geometric.nn.global_mean_pool(x, batch.batch)

            # pass through fingerprint net
            out = U_net(pooled)
            all_p.append(out.cpu())

    return torch.cat(all_p, dim=0)

import os
import torch
from torch_geometric.data import Data
from torch.serialization import add_safe_globals
from torch_geometric.data.data import DataEdgeAttr, DataTensorAttr
from torch_geometric.data.storage import GlobalStorage

# Trust necessary PyG objects during loading
add_safe_globals([Data, DataEdgeAttr, DataTensorAttr, GlobalStorage])

def load_flat_variants(folder):
    files = sorted(os.listdir(folder))
    graphs = []
    for f in files:
        if f.endswith('.pt'):
            path = os.path.join(folder, f)
            graphs.append(torch.load(path, map_location='cpu'))  # or device
    return graphs

variant_dir = "/content/drive/MyDrive/gnnfingers/variants/Cora_SAGEMean"
graphs = load_flat_variants(variant_dir)
print(f"‚úÖ Loaded {len(graphs)} graphs (should be 400: 200 P + 200 N)")

from torch_geometric.loader import DataLoader
import torch.nn.functional as F

def extract_fp(model, U_net, graphs, batch_size=32):
    model.eval()
    U_net.eval()
    all_p = []

    loader = DataLoader(graphs, batch_size=batch_size, shuffle=False)
    for batch in loader:
        batch = batch.to(device)
        x = F.relu(model.conv1(batch.x, batch.edge_index))
        x = F.dropout(x, p=0.5, training=False)
        x = model.conv2(x, batch.edge_index)
        pooled = torch_geometric.nn.global_mean_pool(x, batch.batch)
        logits = U_net(pooled)
        all_p.append(logits.cpu())

    return torch.cat(all_p, dim=0)

import torch
import torch.nn.functional as F
from torch_geometric.loader import DataLoader  # ‚úÖ updated import
from torch_geometric.nn import global_mean_pool

def extract_fp(model, U_net, graphs, device, batch_size=32):
    model.eval()
    U_net.eval()
    all_p = []

    loader = DataLoader(graphs, batch_size=batch_size, shuffle=False)
    for batch in loader:
        batch = batch.to(device)
        x = F.relu(model.conv1(batch.x, batch.edge_index))
        x = F.dropout(x, p=0.5, training=False)
        x = model.conv2(x, batch.edge_index)
        pooled = global_mean_pool(x, batch.batch)
        logits = U_net(pooled)
        all_p.append(logits.cpu())

    return torch.cat(all_p, dim=0)

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch_geometric.nn import SAGEConv, global_mean_pool
from torch_geometric.loader import DataLoader

class SAGEMean(torch.nn.Module):
    def __init__(self, in_channels, hidden_channels, out_channels):
        super().__init__()
        self.conv1 = SAGEConv(in_channels, hidden_channels, aggr='mean')
        self.conv2 = SAGEConv(hidden_channels, out_channels, aggr='mean')

    def forward(self, x, edge_index, batch):
        x = F.relu(self.conv1(x, edge_index))
        x = self.conv2(x, edge_index)
        x = global_mean_pool(x, batch)
        return x

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

victim = SAGEMean(in_channels=1433, hidden_channels=64, out_channels=64).to(device)
victim.load_state_dict(torch.load(
    "/content/drive/MyDrive/gnnfingers/victims/victim_Cora_SAGEMean.pth",
    map_location=device
), strict=False)
victim.eval()

print("‚úÖ Victim model loaded.")

class FingerprintNetMLP(nn.Module):
    def __init__(self, input_dim=2, hidden_dim=16):
        super(FingerprintNetMLP, self).__init__()
        self.net = nn.Sequential(
            nn.Linear(input_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, 2)
        )

    def forward(self, x):
        return self.net(x)

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

victim = SAGEMean(in_channels=1433, hidden_channels=64, out_channels=64).to(device)
victim.load_state_dict(torch.load("/content/drive/MyDrive/gnnfingers/victims/victim_Cora_SAGEMean.pth", map_location=device))
victim.eval()

fp_net = FingerprintNetMLP(input_dim=2, hidden_dim=16).to(device)
fp_net.load_state_dict(torch.load("/content/drive/MyDrive/gnnfingers/checkpoints/U_Cora_SAGEMean.pth", map_location=device))
fp_net.eval()

from torch.serialization import add_safe_globals
from torch_geometric.data import Data
from torch_geometric.data.storage import GlobalStorage
add_safe_globals([Data, GlobalStorage])

import os, torch

def load_flat_variants(folder):
    files = sorted(os.listdir(folder))
    return [torch.load(os.path.join(folder, f), weights_only=False) for f in files]

graphs = load_flat_variants("/content/drive/MyDrive/gnnfingers/variants/Cora_SAGEMean")

from torch_geometric.loader import DataLoader
import torch.nn.functional as F
from torch_geometric.nn import global_mean_pool

def extract_fp(model, U_net, graphs, batch_size=4, device='cpu'):
    model = model.to(device)
    U_net = U_net.to(device)
    model.eval()
    U_net.eval()

    fingerprints = []
    loader = DataLoader(graphs, batch_size=batch_size, shuffle=False)

    with torch.no_grad():
        for batch in loader:
            batch = batch.to(device)
            x = F.relu(model.conv1(batch.x, batch.edge_index))
            x = F.dropout(x, p=0.5, training=False)
            x = model.conv2(x, batch.edge_index)
            pooled = global_mean_pool(x, batch.batch)
            logits = U_net(pooled)
            fingerprints.append(logits.cpu())

    return torch.cat(fingerprints, dim=0)

import os
import torch
from torch_geometric.data import Data

def load_variants(directory):
    """Load all PyTorch Geometric graph objects (.pt) from a directory"""
    graphs = []
    for fname in sorted(os.listdir(directory)):
        if fname.endswith(".pt"):
            fpath = os.path.join(directory, fname)
            try:
                graph = torch.load(fpath, map_location='cpu')
                if isinstance(graph, Data):
                    graphs.append(graph)
            except Exception as e:
                print(f"‚ùå Failed to load {fname}: {e}")
    return graphs

from utils import load_variants  # or however you defined load_variants

variant_dir = "/content/drive/MyDrive/gnnfingers/variants/Cora_SAGEMean"
graphs = load_variants(variant_dir)
print(f"‚úÖ Loaded {len(graphs)} graphs")

from torch_geometric.loader import DataLoader
import torch.nn.functional as F
from torch_geometric.nn import global_mean_pool

def extract_fp(model, U_net, graphs, batch_size=4, device='cpu'):
    model = model.to(device)
    U_net = U_net.to(device)
    model.eval()
    U_net.eval()

    fingerprints = []
    loader = DataLoader(graphs, batch_size=batch_size, shuffle=False)

    with torch.no_grad():
        for batch in loader:
            batch = batch.to(device)
            x = F.relu(model.conv1(batch.x, batch.edge_index))
            x = F.dropout(x, p=0.5, training=False)
            x = model.conv2(x, batch.edge_index)
            pooled = global_mean_pool(x, batch.batch)
            logits = U_net(pooled)
            fingerprints.append(logits.cpu())

    return torch.cat(fingerprints, dim=0)

device = 'cpu'  # or 'cuda' if you're stable
chunk_size = 50
all_fps = []

for i in range(0, len(graphs), chunk_size):
    print(f"Processing graphs {i} to {i+chunk_size}...")
    chunk = graphs[i:i + chunk_size]
    fps_chunk = extract_fp(victim, fp_net, chunk, batch_size=4, device=device)
    all_fps.append(fps_chunk)

fingerprints = torch.cat(all_fps, dim=0)
torch.save(fingerprints, "/content/drive/MyDrive/gnnfingers/fingerprints/Cora_SAGEMean.pt")
print(f"‚úÖ Fingerprints saved with shape: {fingerprints.shape}")

fingerprints = torch.cat(all_fps, dim=0)

del all_fps
del graphs
torch.cuda.empty_cache()  # if using GPU
import gc; gc.collect()   # force Python garbage collection

fingerprints = torch.load("/content/drive/MyDrive/gnnfingers/fingerprints/Cora_SAGEMean.pt")
print(fingerprints.shape)

import torch
import torch.nn as nn
import torch.optim as optim
from sklearn.metrics import roc_auc_score, roc_curve, auc
import matplotlib.pyplot as plt

# Load saved fingerprints
fingerprints = torch.load("/content/drive/MyDrive/gnnfingers/fingerprints/Cora_SAGEMean.pt")

# Create labels
labels = torch.cat([
    torch.ones(200),     # pirated (positive)
    torch.zeros(200)     # irrelevant (negative)
]).long()

print(f"‚úÖ Loaded {fingerprints.shape[0]} fingerprints")

class Univerifier(nn.Module):
    def __init__(self):
        super().__init__()
        self.classifier = nn.Sequential(
            nn.Linear(2, 64),
            nn.LeakyReLU(),
            nn.Linear(64, 32),
            nn.LeakyReLU(),
            nn.Linear(32, 2)  # output logits for 2 classes
        )

    def forward(self, x):
        return self.classifier(x)

from torch.utils.data import TensorDataset, DataLoader

# Create DataLoader
dataset = TensorDataset(fingerprints, labels)
loader = DataLoader(dataset, batch_size=32, shuffle=True)

model = Univerifier()
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)

# Training loop
for epoch in range(20):
    model.train()
    total_loss = 0
    for batch_x, batch_y in loader:
        logits = model(batch_x)
        loss = criterion(logits, batch_y)

        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        total_loss += loss.item()

    print(f"Epoch {epoch+1:02d} - Loss: {total_loss:.4f}")

model.eval()
with torch.no_grad():
    logits = model(fingerprints)
    probs = torch.softmax(logits, dim=1)[:, 1]  # probability of class 1 (pirated)

# Compute metrics
fpr, tpr, thresholds = roc_curve(labels.numpy(), probs.numpy())
aruc = auc(fpr, tpr)
print(f"ARUC: {aruc:.4f}")

# Plot ROC curve
plt.figure()
plt.plot(fpr, tpr, label=f"ARUC = {aruc:.4f}")
plt.plot([0, 1], [0, 1], linestyle='--')
plt.xlabel("False Positive Rate (1 - Uniqueness)")
plt.ylabel("True Positive Rate (Robustness)")
plt.title("ROC Curve for Univerifier")
plt.legend()
plt.grid(True)
plt.show()

import torch.nn as nn

class FingerprintNetMLP(nn.Module):
    def __init__(self, in_dim=64, hidden_dim=32, out_dim=2):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(in_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, out_dim)
        )

    def forward(self, x):
        return self.net(x)

class Univerifier(nn.Module):
    def __init__(self, in_dim=2):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(in_dim, 64),
            nn.LeakyReLU(),
            nn.Linear(64, 32),
            nn.LeakyReLU(),
            nn.Linear(32, 2)
        )

    def forward(self, x):
        return self.net(x)

from torch_geometric.loader import DataLoader
from torch_geometric.nn import global_mean_pool
import torch.nn.functional as F

def extract_pooled_features(model, graphs, batch_size=8, device='cpu'):
    model = model.to(device)
    model.eval()
    features = []

    loader = DataLoader(graphs, batch_size=batch_size, shuffle=False)

    with torch.no_grad():
        for batch in loader:
            batch = batch.to(device)
            x = F.relu(model.conv1(batch.x, batch.edge_index))
            x = model.conv2(x, batch.edge_index)
            pooled = global_mean_pool(x, batch.batch)
            features.append(pooled.cpu())

    return torch.cat(features, dim=0)

variant_dir = "/content/drive/MyDrive/gnnfingers/variants/Cora_SAGEMean"
graphs = load_variants(variant_dir)
pooled_features = extract_pooled_features(victim, graphs, batch_size=8, device='cpu')

labels = torch.cat([
    torch.ones(200),  # Pirated (F+)
    torch.zeros(200)  # Irrelevant (F-)
]).long()

print(pooled_features.shape)  # Should be [400, 64]

fp_net = FingerprintNetMLP(in_dim=64, hidden_dim=32, out_dim=2)
univerifier = Univerifier(in_dim=2)

optimizer = torch.optim.Adam(list(fp_net.parameters()) + list(univerifier.parameters()), lr=0.001)
criterion = nn.CrossEntropyLoss()

from torch.utils.data import DataLoader, TensorDataset

dataset = TensorDataset(pooled_features, labels)
loader = DataLoader(dataset, batch_size=32, shuffle=True)

for epoch in range(20):
    fp_net.train()
    univerifier.train()
    total_loss = 0

    for x_batch, y_batch in loader:
        z = fp_net(x_batch)
        logits = univerifier(z)
        loss = criterion(logits, y_batch)

        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        total_loss += loss.item()

    print(f"Epoch {epoch+1:02d} - Loss: {total_loss:.4f}")

fp_net.eval()
univerifier.eval()
with torch.no_grad():
    z = fp_net(pooled_features)
    probs = torch.softmax(univerifier(z), dim=1)[:, 1]  # pirated class

from sklearn.metrics import roc_curve, auc
import matplotlib.pyplot as plt

fpr, tpr, _ = roc_curve(labels.numpy(), probs.numpy())
aruc = auc(fpr, tpr)

print(f"üîê ARUC: {aruc:.4f}")

plt.figure()
plt.plot(fpr, tpr, label=f"ARUC = {aruc:.4f}")
plt.plot([0, 1], [0, 1], linestyle='--', color='gray')
plt.xlabel("False Positive Rate (1 - Uniqueness)")
plt.ylabel("True Positive Rate (Robustness)")
plt.title("ROC Curve after Joint Training (Cora/SAGEMean)")
plt.legend()
plt.grid(True)
plt.show()

import os
import torch
from torch_geometric.data import Data
from torch_geometric.loader import DataLoader
from torch_geometric.nn import global_mean_pool
import torch.nn.functional as F

def pooled_from_model_graph(model_path, graph, model_class, device='cpu'):
    model = model_class(in_channels=1433, hidden_channels=64, out_channels=7)
    model.load_state_dict(torch.load(model_path, map_location=device), strict=False)
    model = model.to(device)
    model.eval()

    graph = graph.to(device)
    with torch.no_grad():
        x = F.relu(model.conv1(graph.x, graph.edge_index))
        x = model.conv2(x, graph.edge_index)
        pooled = global_mean_pool(x, graph.batch if hasattr(graph, 'batch') else torch.zeros(x.size(0), dtype=torch.long))
    return pooled.cpu()

!find /content/drive/MyDrive/gnnfingers/ -type f -name "*.pth" | head -20

!ls /content/drive/MyDrive/gnnfingers/variants/ENZ_GCNMean/positive

import torch
import torch.nn.functional as F
from torch_geometric.nn import GCNConv, global_mean_pool

class GCNMean(torch.nn.Module):
    def __init__(self, in_channels, hidden_channels, out_channels):
        super(GCNMean, self).__init__()
        self.conv1 = GCNConv(in_channels, hidden_channels)
        self.conv2 = GCNConv(hidden_channels, out_channels)

    def forward(self, data):
        x, edge_index, batch = data.x, data.edge_index, data.batch
        x = F.relu(self.conv1(x, edge_index))
        x = self.conv2(x, edge_index)
        return global_mean_pool(x, batch)

graphs = load_variants("/content/drive/MyDrive/gnnfingers/variants/ENZ_GCNMean")

print(f"# of ft_*.pth models: {len(model_files)}")
print(f"# of graphs loaded: {len(graphs)}")

torch.save(graphs, "/content/drive/MyDrive/gnnfingers/variants/ENZ_GCNMean/graphs_ft_only.pt")

graphs = torch.load("/content/drive/MyDrive/gnnfingers/variants/ENZ_GCNMean/graphs_ft_only.pt")

import os
import torch
from torch_geometric.datasets import Planetoid
from torch_geometric.nn import GCNConv
from torch_geometric.loader import DataLoader
from torch_geometric.nn import global_mean_pool
import torch.nn.functional as F

# Define your GCN model
class GCNMean(torch.nn.Module):
    def __init__(self, in_channels, hidden_channels, out_channels):
        super(GCNMean, self).__init__()
        self.conv1 = GCNConv(in_channels, hidden_channels)
        self.conv2 = GCNConv(hidden_channels, out_channels)

    def forward(self, data):
        x, edge_index = data.x, data.edge_index
        batch = data.batch if hasattr(data, 'batch') else torch.zeros(x.size(0), dtype=torch.long, device=x.device)
        x = F.relu(self.conv1(x, edge_index))
        x = self.conv2(x, edge_index)
        return global_mean_pool(x, batch)

# Load Cora dataset
dataset = Planetoid(root='/tmp/Cora', name='Cora')
data = dataset[0]

# Save directory
save_dir = "/content/drive/MyDrive/gnnfingers/variants/ENZ_GCNMean"
os.makedirs(save_dir, exist_ok=True)

# Generate 30 fine-tuned models and save their graphs
# Train using node-level output for classification
for epoch in range(50):
    model.train()
    optimizer.zero_grad()

    x = F.relu(model.conv1(data.x, data.edge_index))
    out = model.conv2(x, data.edge_index)  # shape: [2708, 7]

    loss = F.cross_entropy(out[data.train_mask], data.y[data.train_mask])
    loss.backward()
    optimizer.step()

    # Save model
    model_path = os.path.join(save_dir, f"ft_{i:03d}.pth")
    torch.save(model.state_dict(), model_path)

    # Save graph
    graph_path = os.path.join(save_dir, f"ft_{i:03d}.pt")
    torch.save(data, graph_path)

    print(f"‚úÖ Saved ft_{i:03d}.pth and .pt")

from tqdm import tqdm

device = 'cuda' if torch.cuda.is_available() else 'cpu'

all_features = []
labels = []

model_dir = "/content/drive/MyDrive/gnnfingers/variants/ENZ_GCNMean"

for i in tqdm(range(30)):
    model_path = os.path.join(model_dir, f"ft_{i:03d}.pth")
    graph_path = os.path.join(model_dir, f"ft_{i:03d}.pt")

    try:
        model = GCNMean(1433, 64, 7).to(device)
        model.load_state_dict(torch.load(model_path, map_location=device), strict=False)
        model.eval()

        graph = torch.load(graph_path).to(device)

        with torch.no_grad():
            x = F.relu(model.conv1(graph.x, graph.edge_index))
            x = model.conv2(x, graph.edge_index)
            batch = graph.batch if hasattr(graph, 'batch') else torch.zeros(x.size(0), dtype=torch.long, device=x.device)
            pooled = global_mean_pool(x, batch)
            all_features.append(pooled.cpu())
            labels.append(1)

        print(f"‚úÖ Processed ft_{i:03d}")

    except Exception as e:
        print(f"‚ö†Ô∏è Error in ft_{i:03d}: {e}")

# Save the fingerprints and labels
fingerprints = torch.cat(all_features, dim=0)
labels = torch.tensor(labels)

torch.save(fingerprints, os.path.join(model_dir, "Cora_GCNMean_ft_only.pt"))
torch.save(labels, os.path.join(model_dir, "Cora_GCNMean_ft_labels.pt"))

print(f"‚úÖ Saved fingerprints with shape: {fingerprints.shape}")

from torch_geometric.datasets import Planetoid
from torch_geometric.nn import GCNConv, global_mean_pool
import torch.nn.functional as F
import torch.nn as nn
import torch
import os
from tqdm import tqdm

# Model definition
class GCNMean(nn.Module):
    def __init__(self, in_channels, hidden_channels, out_channels):
        super().__init__()
        self.conv1 = GCNConv(in_channels, hidden_channels)
        self.conv2 = GCNConv(hidden_channels, out_channels)

    def forward(self, data):
        x = F.relu(self.conv1(data.x, data.edge_index))
        x = self.conv2(x, data.edge_index)
        return x

# Detect device
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Using device: {device}")

# Load dataset
dataset = Planetoid(root='/tmp/Cora', name='Cora')
data = dataset[0].to(device)

# Save location
save_dir = "/content/drive/MyDrive/gnnfingers/variants/ENZ_GCNMean"
os.makedirs(save_dir, exist_ok=True)

# Train and save 30 fine-tuned models and graphs
for i in tqdm(range(30)):
    model = GCNMean(dataset.num_node_features, 64, dataset.num_classes).to(device)
    optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)

    model.train()
    for epoch in range(30):
        optimizer.zero_grad()
        out = model(data)
        loss = F.cross_entropy(out[data.train_mask], data.y[data.train_mask])
        loss.backward()
        optimizer.step()

    # Save model + graph
    torch.save(model.state_dict(), os.path.join(save_dir, f"ft_{i:03d}.pth"))
    torch.save(data.cpu(), os.path.join(save_dir, f"ft_{i:03d}.pt"))

import torch
import os
from tqdm import tqdm
from torch_geometric.nn import global_mean_pool
from torch_geometric.data import Data
import torch.nn.functional as F
import torch.nn as nn
from torch_geometric.nn import GCNConv

# Define GCNMean again
class GCNMean(nn.Module):
    def __init__(self, in_channels, hidden_channels, out_channels):
        super().__init__()
        self.conv1 = GCNConv(in_channels, hidden_channels)
        self.conv2 = GCNConv(hidden_channels, out_channels)

    def forward(self, data):
        x = F.relu(self.conv1(data.x, data.edge_index))
        x = self.conv2(x, data.edge_index)
        return x

    def pooled_output(self, data):
        x = F.relu(self.conv1(data.x, data.edge_index))
        x = self.conv2(x, data.edge_index)
        batch = data.batch if hasattr(data, 'batch') else torch.zeros(x.size(0), dtype=torch.long, device=x.device)
        return global_mean_pool(x, batch)

# Set device
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Path setup
variant_dir = "/content/drive/MyDrive/gnnfingers/variants/ENZ_GCNMean"
model_files = sorted([f for f in os.listdir(variant_dir) if f.startswith("ft_") and f.endswith(".pth")])
graph_files = sorted([f for f in os.listdir(variant_dir) if f.startswith("ft_") and f.endswith(".pt")])

# Setup
fingerprints = []
labels = []

for i in tqdm(range(len(model_files))):
    model_path = os.path.join(variant_dir, model_files[i])
    graph_path = os.path.join(variant_dir, graph_files[i])

    try:
        model = GCNMean(1433, 64, 7).to(device)  # Cora dimensions
        model.load_state_dict(torch.load(model_path, map_location=device))
        model.eval()

        data = torch.load(graph_path).to(device)
        with torch.no_grad():
            fp = model.pooled_output(data)
            fingerprints.append(fp.cpu())
            labels.append(1)  # label 1 = positive (ft variant)

    except Exception as e:
        print(f"‚ö†Ô∏è Error in {model_files[i]}: {e}")

# Save results
fingerprints = torch.cat(fingerprints, dim=0)
labels = torch.tensor(labels)

torch.save(fingerprints, "/content/drive/MyDrive/gnnfingers/fingerprints/Cora_GCNMean_ft_only.pt")
torch.save(labels, "/content/drive/MyDrive/gnnfingers/fingerprints/Cora_GCNMean_ft_labels.pt")

print(f"‚úÖ Saved fingerprints with shape: {fingerprints.shape}")

import torch
from tqdm import tqdm
from torch_geometric.datasets import Planetoid
from torch_geometric.nn import global_mean_pool
from torch_geometric.data import Data
import torch.nn.functional as F
import torch.nn as nn
from torch_geometric.nn import GCNConv

# GCNMean model (same as before)
class GCNMean(nn.Module):
    def __init__(self, in_channels, hidden_channels, out_channels):
        super().__init__()
        self.conv1 = GCNConv(in_channels, hidden_channels)
        self.conv2 = GCNConv(hidden_channels, out_channels)

    def forward(self, data):
        x = F.relu(self.conv1(data.x, data.edge_index))
        x = self.conv2(x, data.edge_index)
        return x

    def pooled_output(self, data):
        x = F.relu(self.conv1(data.x, data.edge_index))
        x = self.conv2(x, data.edge_index)
        batch = data.batch if hasattr(data, 'batch') else torch.zeros(x.size(0), dtype=torch.long, device=x.device)
        return global_mean_pool(x, batch)

# Load Cora graph
dataset = Planetoid(root='/tmp/Cora', name='Cora')
cora_data = dataset[0]

# Set device
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
cora_data = cora_data.to(device)

# Generate 30 random GCNMean fingerprints
neg_fps_random = []

for _ in tqdm(range(30)):
    model = GCNMean(1433, 64, 7).to(device)
    model.eval()
    with torch.no_grad():
        fp = model.pooled_output(cora_data)
        neg_fps_random.append(fp.cpu())

neg_fps_random = torch.cat(neg_fps_random, dim=0)
neg_labels_random = torch.zeros(30)

torch.save(neg_fps_random, "/content/drive/MyDrive/gnnfingers/fingerprints/Cora_GCNMean_neg_random.pt")
torch.save(neg_labels_random, "/content/drive/MyDrive/gnnfingers/fingerprints/Cora_GCNMean_neg_random_labels.pt")
print("‚úÖ Saved 30 random negative fingerprints.")

from torch_geometric.datasets import TUDataset

# Load 30 PROTEINS graphs
proteins = TUDataset(root='/tmp/PROTEINS', name='PROTEINS')
neg_fps_proteins = []

model = GCNMean(proteins.num_node_features, 64, 2).to(device)
model.eval()

for i in tqdm(range(30)):
    g = proteins[i].to(device)
    with torch.no_grad():
        fp = model.pooled_output(g)
        neg_fps_proteins.append(fp.cpu())

neg_fps_proteins = torch.cat(neg_fps_proteins, dim=0)
neg_labels_proteins = torch.zeros(30)

torch.save(neg_fps_proteins, "/content/drive/MyDrive/gnnfingers/fingerprints/GCNMean_neg_proteins.pt")
torch.save(neg_labels_proteins, "/content/drive/MyDrive/gnnfingers/fingerprints/GCNMean_neg_proteins_labels.pt")
print("‚úÖ Saved 30 irrelevant-graph negative fingerprints.")

class Univerifier(nn.Module):
    def __init__(self, input_dim):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(input_dim, 32),
            nn.ReLU(),
            nn.Linear(32, 1)
        )

    def forward(self, x):
        return self.net(x)

print("Positive fingerprint shape:", pos_fp.shape)
print("Random negative fingerprint shape:", neg_fp_random.shape)
print("Proteins negative fingerprint shape:", neg_fp_proteins.shape)

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch_geometric.nn import GCNConv, global_mean_pool

# GCNMean Model (used for Cora and Proteins)
class GCNMean(nn.Module):
    def __init__(self, in_channels, hidden_channels, out_channels):
        super(GCNMean, self).__init__()
        self.conv1 = GCNConv(in_channels, hidden_channels)
        self.conv2 = GCNConv(hidden_channels, out_channels)
        self.dropout = 0.5

    def forward(self, x, edge_index, batch=None):
        x = F.relu(self.conv1(x, edge_index))
        x = F.dropout(x, p=self.dropout, training=self.training)
        x = self.conv2(x, edge_index)
        if batch is not None:
            x = global_mean_pool(x, batch)
        return x

# FingerprintNetMLP used for mapping pooled embeddings to fingerprints
class FingerprintNetMLP(nn.Module):
    def __init__(self, in_dim=7, out_dim=7):  # default for Cora
        super(FingerprintNetMLP, self).__init__()
        self.net = nn.Sequential(
            nn.Linear(in_dim, 128),
            nn.ReLU(),
            nn.Linear(128, out_dim)
        )

    def forward(self, x):
        return self.net(x)

with open("/content/drive/MyDrive/gnnfingers/models.py", "r") as f:
    print(f.read())

from models import SAGEMean, FingerprintNetMLP

from torch_geometric.nn import GCNConv

class GCNMean(nn.Module):
    def __init__(self, in_channels=1433, hidden_channels=128, out_channels=7, dropout=0.5):
        super(GCNMean, self).__init__()
        self.conv1 = GCNConv(in_channels, hidden_channels)
        self.conv2 = GCNConv(hidden_channels, hidden_channels)
        self.dropout = dropout
        self.classifier = nn.Linear(hidden_channels, out_channels)

    def forward(self, x, edge_index, batch=None):
        x = self.conv1(x, edge_index)
        x = F.relu(x)
        x = F.dropout(x, p=self.dropout, training=self.training)
        x = self.conv2(x, edge_index)
        if batch is not None:
            x = global_mean_pool(x, batch)
        return self.classifier(x)

from google.colab import drive
drive.mount('/content/drive')

import sys
sys.path.append('/content/drive/MyDrive/gnnfingers')

# Check the exact file path and contents
!ls /content/drive/MyDrive/gnnfingers/
!cat /content/drive/MyDrive/gnnfingers/models.py

from models import SAGEMean, FingerprintNetMLP

from torch_geometric.nn import GCNConv

class GCNMean(nn.Module):
    def __init__(self, in_channels=1433, hidden_channels=128, out_channels=7, dropout=0.5):
        super(GCNMean, self).__init__()
        self.conv1 = GCNConv(in_channels, hidden_channels)
        self.conv2 = GCNConv(hidden_channels, hidden_channels)
        self.dropout = dropout
        self.classifier = nn.Linear(hidden_channels, out_channels)

    def forward(self, x, edge_index, batch=None):
        x = self.conv1(x, edge_index)
        x = F.relu(x)
        x = F.dropout(x, p=self.dropout, training=self.training)
        x = self.conv2(x, edge_index)
        if batch is not None:
            x = global_mean_pool(x, batch)
        return self.classifier(x)

model = GCNMean()
print(model)

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch_geometric.nn import GCNConv, SAGEConv, global_mean_pool

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# GCNMean Model (used for Cora)
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
class GCNMean(nn.Module):
    def __init__(self, in_channels=1433, hidden_channels=128, out_channels=7, dropout=0.5):
        super(GCNMean, self).__init__()
        self.conv1 = GCNConv(in_channels, hidden_channels)
        self.conv2 = GCNConv(hidden_channels, hidden_channels)
        self.dropout = dropout
        self.classifier = nn.Linear(hidden_channels, out_channels)

    def forward(self, x, edge_index, batch=None):
        x = self.conv1(x, edge_index)
        x = F.relu(x)
        x = F.dropout(x, p=self.dropout, training=self.training)
        x = self.conv2(x, edge_index)
        if batch is not None:
            x = global_mean_pool(x, batch)
        return self.classifier(x)

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# SAGEMean Model (used for other variants)
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
class SAGEMean(nn.Module):
    def __init__(self, in_channels=1433, hidden_channels=128, out_channels=7, dropout=0.5):
        super(SAGEMean, self).__init__()
        self.conv1 = SAGEConv(in_channels, hidden_channels)
        self.conv2 = SAGEConv(hidden_channels, hidden_channels)
        self.dropout = dropout
        self.classifier = nn.Linear(hidden_channels, out_channels)

    def forward(self, x, edge_index, batch=None):
        x = self.conv1(x, edge_index)
        x = F.relu(x)
        x = F.dropout(x, p=self.dropout, training=self.training)
        x = self.conv2(x, edge_index)
        if batch is not None:
            x = global_mean_pool(x, batch)
        return self.classifier(x)

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# FingerprintNet MLP
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
class FingerprintNetMLP(nn.Module):
    def __init__(self, input_dim=128, hidden_dim=64, output_dim=2):
        super(FingerprintNetMLP, self).__init__()
        self.mlp = nn.Sequential(
            nn.Linear(input_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, output_dim)
        )

    def forward(self, x):
        return self.mlp(x)

from google.colab import drive
drive.mount('/content/drive')

!ls /content/drive/MyDrive/gnnfingers

import sys
import importlib

# Add the directory containing models.py to the system path
sys.path.append('/content/drive/MyDrive/gnnfingers')

# Clear previous import if any
if 'models' in sys.modules:
    del sys.modules['models']

# Now import the modules freshly
from models import SAGEMean, FingerprintNetMLP  # Use GCNMean if defined instead

import torch
from torch_geometric.datasets import TUDataset
from torch_geometric.loader import DataLoader
from torch_geometric.nn import global_mean_pool
from tqdm import tqdm

from models import SAGEMean, FingerprintNetMLP

# Load PROTEINS dataset
dataset = TUDataset(root='/content/drive/MyDrive/gnnfingers/PROTEINS', name='PROTEINS')
loader = DataLoader(dataset, batch_size=1)

# Set device
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

# Initialize victim and fingerprint model
victim = SAGEMean(in_channels=dataset.num_features, hidden_channels=128, out_channels=7).to(device)
fp_net = FingerprintNetMLP(input_dim=128, hidden_dim=64).to(device)

fingerprints = []

for batch in tqdm(loader):
    batch = batch.to(device)

    with torch.no_grad():
        # Get pooled representation from SAGEMean
        x = victim.conv1(batch.x, batch.edge_index)
        x = torch.relu(x)
        x = victim.conv2(x, batch.edge_index)
        pooled = global_mean_pool(x, batch.batch)

        # Get fingerprint
        fp = fp_net(pooled)
        fingerprints.append(fp.cpu())

# Stack and save
fingerprints = torch.cat(fingerprints, dim=0)
torch.save(fingerprints, '/content/drive/MyDrive/gnnfingers/fingerprints/neg_fp_proteins.pt')

print("‚úÖ Saved fingerprints with shape:", fingerprints.shape)

import os

fp_dir = "/content/drive/MyDrive/gnnfingers/fingerprints"
print("Files in fingerprints directory:")
print(os.listdir(fp_dir))

import torch

# Load fingerprints
pos_fp = torch.load('/content/drive/MyDrive/gnnfingers/fingerprints/fps_Cora_SAGEMean.pt')
neg_fp_random = torch.load('/content/drive/MyDrive/gnnfingers/fingerprints/Cora_GCNMean_neg_random.pt')
neg_fp_proteins = torch.load('/content/drive/MyDrive/gnnfingers/fingerprints/neg_fp_proteins.pt')

print(f"‚úÖ Positive fingerprint shape: {pos_fp.shape}")
print(f"‚úÖ Random negative fingerprint shape: {neg_fp_random.shape}")
print(f"‚úÖ Proteins negative fingerprint shape: {neg_fp_proteins.shape}")

import torch
import numpy as np
from sklearn.manifold import TSNE
import matplotlib.pyplot as plt

# Load your original fingerprints
pos_fp = torch.load('/content/drive/MyDrive/gnnfingers/fingerprints/fps_Cora_SAGEMean.pt')            # (400, 64)
neg_fp_random = torch.load('/content/drive/MyDrive/gnnfingers/fingerprints/Cora_GCNMean_neg_random.pt')  # (30, 7)
neg_fp_proteins = torch.load('/content/drive/MyDrive/gnnfingers/fingerprints/neg_fp_proteins.pt')        # (1113, 2)

# Pad to 64 dimensions
def pad_tensor(t, target_dim=64):
    return F.pad(t, (0, target_dim - t.shape[1]), "constant", 0)

neg_fp_random_padded = pad_tensor(neg_fp_random, 64)
neg_fp_proteins_padded = pad_tensor(neg_fp_proteins, 64)

# Combine all
X_all = torch.cat([pos_fp, neg_fp_random_padded, neg_fp_proteins_padded], dim=0).numpy()
y_all = np.array([1]*len(pos_fp) + [0]*len(neg_fp_random_padded) + [0]*len(neg_fp_proteins_padded))

# Run t-SNE with low perplexity
tsne = TSNE(n_components=2, perplexity=20, n_iter=1000, init='random', learning_rate='auto', random_state=42)
X_tsne = tsne.fit_transform(X_all)

# Plot
plt.figure(figsize=(8,6))
plt.scatter(X_tsne[y_all == 1, 0], X_tsne[y_all == 1, 1], c='blue', label='Positive (Cora)', alpha=0.6)
plt.scatter(X_tsne[:len(pos_fp), 0], X_tsne[:len(pos_fp), 1], c='blue', label='Positive (Cora)', alpha=0.6)
plt.scatter(X_tsne[len(pos_fp):len(pos_fp)+len(neg_fp_random_padded), 0],
            X_tsne[len(pos_fp):len(pos_fp)+len(neg_fp_random_padded), 1],
            c='red', label='Negatives (Random)', alpha=0.6)
plt.scatter(X_tsne[-len(neg_fp_proteins_padded):, 0],
            X_tsne[-len(neg_fp_proteins_padded):, 1],
            c='green', label='Negatives (Proteins)', alpha=0.6)
plt.legend()
plt.title('t-SNE of Fingerprints')
plt.grid(True)
plt.show()

import torch
from sklearn.metrics import roc_auc_score, confusion_matrix

# Simulate small dataset
torch.manual_seed(42)
pos_fp = torch.randn(10, 64)
neg_fp_random = torch.randn(5, 64)
neg_fp_proteins = torch.randn(5, 64)

# Concatenate
X = torch.cat([pos_fp, neg_fp_random, neg_fp_proteins], dim=0)
y = torch.cat([torch.ones(len(pos_fp)), torch.zeros(len(neg_fp_random) + len(neg_fp_proteins))])

# Mock classifier: dot product with random weights
weights = torch.randn(64)
logits = X @ weights
probs = torch.sigmoid(logits)

# AUROC
auroc = roc_auc_score(y.numpy(), probs.detach().numpy())

# Confusion matrix at 0.5 threshold
y_pred = (probs >= 0.5).long()
conf_mat = confusion_matrix(y.numpy(), y_pred.numpy())

print(" AUROC:", round(auroc, 4))
print(" Confusion Matrix:\n", conf_mat)